{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Experiment 1 (AlBERT transformer + CL2(GAN training))**","metadata":{}},{"cell_type":"markdown","source":"1. Clone the repository\n2. Make the following changes in model.py, pretrain_damsm.py and trainer.py files.\n3. Follow the same procedure for DAMSM training, GAN training and evaluation.","metadata":{}},{"cell_type":"code","source":" !git clone https://github.com/mansivv9/Text_to_image_synthesis_Major_Project.git","metadata":{"execution":{"iopub.status.busy":"2023-05-11T09:06:25.630509Z","iopub.execute_input":"2023-05-11T09:06:25.630854Z","iopub.status.idle":"2023-05-11T09:06:27.125372Z","shell.execute_reply.started":"2023-05-11T09:06:25.630826Z","shell.execute_reply":"2023-05-11T09:06:27.124221Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'Text_to_image_synthesis_Major_Project'...\nremote: Enumerating objects: 194, done.\u001b[K\nremote: Counting objects: 100% (108/108), done.\u001b[K\nremote: Compressing objects: 100% (86/86), done.\u001b[K\nremote: Total 194 (delta 37), reused 69 (delta 22), pack-reused 86\u001b[K\nReceiving objects: 100% (194/194), 458.99 KiB | 7.78 MiB/s, done.\nResolving deltas: 100% (60/60), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.chdir('/kaggle/working/Text_to_image_synthesis_Major_Project/AttnGAN+CL+SN+RoBERTa(AttnGAN_V2)')","metadata":{"execution":{"iopub.status.busy":"2023-05-11T09:06:33.331905Z","iopub.execute_input":"2023-05-11T09:06:33.332291Z","iopub.status.idle":"2023-05-11T09:06:33.338688Z","shell.execute_reply.started":"2023-05-11T09:06:33.332262Z","shell.execute_reply":"2023-05-11T09:06:33.337556Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"%%writefile code/model.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nfrom torch.autograd import Variable\nfrom torchvision import models\nimport torch.utils.model_zoo as model_zoo\nimport torch.nn.functional as F\n\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nfrom miscc.config import cfg\nfrom GlobalAttention import GlobalAttentionGeneral as ATT_NET\n\n# import spacy\n# spacy.prefer_gpu()\n# torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n############################## Bert ##################################################\n\n# class BERT_encoder:    \n#     def __init__(self):\n#         # self._init_processors()\n#         self.bert_model = spacy.load(\"en_trf_distilbertbaseuncased_lg\")\n#     def __call__(self, texts):\n#         docs = self.bert_model(texts)\n#         sentence_emb = docs.vector\n#         word_vectors = [w.vector for w in docs]\n#         return sentence_emb, word_vectors\n            \n\n############################## Image Caption ########################################\n#####################################################################################\nclass GLU(nn.Module):\n    def __init__(self):\n        super(GLU, self).__init__()\n\n    def forward(self, x):\n        nc = x.size(1)\n        assert nc % 2 == 0, 'channels dont divide 2!'\n        nc = int(nc/2)\n        return x[:, :nc] * torch.sigmoid(x[:, nc:])\n\nclass Interpolate(nn.Module):\n    def __init__(self, scale_factor, mode, size=None):\n        super(Interpolate, self).__init__()\n        self.interp = nn.functional.interpolate\n        self.scale_factor = scale_factor\n        self.mode = mode\n        self.size = size\n\n    def forward(self, x):\n        x = self.interp(x, scale_factor=self.scale_factor, mode=self.mode, size=self.size)\n        return x\n\n\ndef conv1x1(in_planes, out_planes, bias=False):\n    \"1x1 convolution with padding\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,\n                     padding=0, bias=bias)\n\n\ndef conv3x3(in_planes, out_planes):\n    \"3x3 convolution with padding\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=1,\n                     padding=1, bias=False)\n\n\n# Upsale the spatial size by a factor of 2\ndef upBlock(in_planes, out_planes):\n    block = nn.Sequential(\n        Interpolate(scale_factor=2, mode='nearest'),\n        conv3x3(in_planes, out_planes * 2),\n        nn.BatchNorm2d(out_planes * 2),\n        GLU())\n    return block\n\n\n# Keep the spatial size\ndef Block3x3_relu(in_planes, out_planes):\n    block = nn.Sequential(\n        conv3x3(in_planes, out_planes * 2),\n        nn.BatchNorm2d(out_planes * 2),\n        GLU())\n    return block\n\n\nclass ResBlock(nn.Module):\n    def __init__(self, channel_num):\n        super(ResBlock, self).__init__()\n        self.block = nn.Sequential(\n            conv3x3(channel_num, channel_num * 2),\n            nn.BatchNorm2d(channel_num * 2),\n            GLU(),\n            conv3x3(channel_num, channel_num),\n            nn.BatchNorm2d(channel_num))\n\n    def forward(self, x):\n        residual = x\n        out = self.block(x)\n        out += residual\n        return out\n\n\n# ############## Text2Image Encoder-Decoder #######\nTRANSFORMER_ENCODER = 'albert-base-v2'\nclass RNN_ENCODER(nn.Module):\n    def __init__(self, ntoken, ninput=300, drop_prob=0.5,\n                 nhidden=128, nlayers=1, bidirectional=True):\n        super(RNN_ENCODER, self).__init__()\n        self.n_steps = cfg.TEXT.WORDS_NUM\n        self.ntoken = ntoken  # size of the dictionary\n        self.ninput = ninput  # size of each embedding vector\n        self.drop_prob = drop_prob  # probability of an element to be zeroed\n        self.nlayers = nlayers  # Number of recurrent layers\n        self.bidirectional = bidirectional\n        self.rnn_type = cfg.RNN_TYPE\n        if bidirectional:\n            self.num_directions = 2\n        else:\n            self.num_directions = 1\n        # number of features in the hidden state\n        self.nhidden = nhidden // self.num_directions\n\n        self.define_module()\n        self.init_weights()\n\n    def define_module(self):\n        self.encoder = nn.Embedding(self.ntoken, self.ninput)\n        self.drop = nn.Dropout(self.drop_prob)\n        if self.rnn_type == 'LSTM':\n            # dropout: If non-zero, introduces a dropout layer on\n            # the outputs of each RNN layer except the last layer\n            self.rnn = nn.LSTM(self.ninput, self.nhidden,\n                               self.nlayers, batch_first=True,\n                               dropout=self.drop_prob,\n                               bidirectional=self.bidirectional)\n        elif self.rnn_type == 'GRU':\n            self.rnn = nn.GRU(self.ninput, self.nhidden,\n                              self.nlayers, batch_first=True,\n                              dropout=self.drop_prob,\n                              bidirectional=self.bidirectional)\n        else:\n            raise NotImplementedError\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        # Do not need to initialize RNN parameters, which have been initialized\n        # http://pytorch.org/docs/master/_modules/torch/nn/modules/rnn.html#LSTM\n        # self.decoder.weight.data.uniform_(-initrange, initrange)\n        # self.decoder.bias.data.fill_(0)\n\n    def init_hidden(self, bsz):\n        weight = next(self.parameters()).data\n        if self.rnn_type == 'LSTM':\n            return (Variable(weight.new(self.nlayers * self.num_directions,\n                                        bsz, self.nhidden).zero_()),\n                    Variable(weight.new(self.nlayers * self.num_directions,\n                                        bsz, self.nhidden).zero_()))\n        else:\n            return Variable(weight.new(self.nlayers * self.num_directions,\n                                       bsz, self.nhidden).zero_())\n\n    def forward(self, captions, cap_lens, hidden, mask=None):\n        # input: torch.LongTensor of size batch x n_steps\n        # --> emb: batch x n_steps x ninput\n        emb = self.drop(self.encoder(captions))\n        #\n        # Returns: a PackedSequence object\n        cap_lens = cap_lens.data.tolist()\n        emb = pack_padded_sequence(emb, cap_lens, batch_first=True)\n        # #hidden and memory (num_layers * num_directions, batch, hidden_size):\n        # tensor containing the initial hidden state for each element in batch.\n        # #output (batch, seq_len, hidden_size * num_directions)\n        # #or a PackedSequence object:\n        # tensor containing output features (h_t) from the last layer of RNN\n        output, hidden = self.rnn(emb, hidden)\n        # PackedSequence object\n        # --> (batch, seq_len, hidden_size * num_directions)\n        output = pad_packed_sequence(output, batch_first=True)[0]\n        # output = self.drop(output)\n        # --> batch x hidden_size*num_directions x seq_len\n        words_emb = output.transpose(1, 2)\n        # --> batch x num_directions*hidden_size\n        if self.rnn_type == 'LSTM':\n            sent_emb = hidden[0].transpose(0, 1).contiguous()\n        else:\n            sent_emb = hidden.transpose(0, 1).contiguous()\n        sent_emb = sent_emb.view(-1, self.nhidden * self.num_directions)\n        return words_emb, sent_emb\n\n\nclass CNN_ENCODER(nn.Module):\n    def __init__(self, nef):\n        super(CNN_ENCODER, self).__init__()\n        if cfg.TRAIN.FLAG:\n            self.nef = nef\n        else:\n            self.nef = 768  # define a uniform ranker\n\n        model = models.inception_v3()\n        url = 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'\n        #densenet201': 'https://download.pytorch.org/models/densenet201-c1103571.pth'\n        #models.densenet201()\n        model.load_state_dict(model_zoo.load_url(url))\n        for param in model.parameters():\n            param.requires_grad = False\n        print('Load pretrained model from ', url)\n        # print(model)\n\n        self.define_module(model)\n        self.init_trainable_weights()\n\n    def define_module(self, model):\n        self.Conv2d_1a_3x3 = model.Conv2d_1a_3x3\n        self.Conv2d_2a_3x3 = model.Conv2d_2a_3x3\n        self.Conv2d_2b_3x3 = model.Conv2d_2b_3x3\n        self.Conv2d_3b_1x1 = model.Conv2d_3b_1x1\n        self.Conv2d_4a_3x3 = model.Conv2d_4a_3x3\n        self.Mixed_5b = model.Mixed_5b\n        self.Mixed_5c = model.Mixed_5c\n        self.Mixed_5d = model.Mixed_5d\n        self.Mixed_6a = model.Mixed_6a\n        self.Mixed_6b = model.Mixed_6b\n        self.Mixed_6c = model.Mixed_6c\n        self.Mixed_6d = model.Mixed_6d\n        self.Mixed_6e = model.Mixed_6e\n        self.Mixed_7a = model.Mixed_7a\n        self.Mixed_7b = model.Mixed_7b\n        self.Mixed_7c = model.Mixed_7c\n\n        self.emb_features = conv1x1(768, self.nef)\n        self.emb_cnn_code = nn.Linear(2048, self.nef)\n\n    def init_trainable_weights(self):\n        initrange = 0.1\n        self.emb_features.weight.data.uniform_(-initrange, initrange)\n        self.emb_cnn_code.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, x):\n        features = None\n        # --> fixed-size input: batch x 3 x 299 x 299\n        x = nn.functional.interpolate(x,size=(299, 299), mode='bilinear', align_corners=False)\n        # 299 x 299 x 3\n        x = self.Conv2d_1a_3x3(x)\n        # 149 x 149 x 32\n        x = self.Conv2d_2a_3x3(x)\n        # 147 x 147 x 32\n        x = self.Conv2d_2b_3x3(x)\n        # 147 x 147 x 64\n        x = F.max_pool2d(x, kernel_size=3, stride=2)\n        # 73 x 73 x 64\n        x = self.Conv2d_3b_1x1(x)\n        # 73 x 73 x 80\n        x = self.Conv2d_4a_3x3(x)\n        # 71 x 71 x 192\n\n        x = F.max_pool2d(x, kernel_size=3, stride=2)\n        # 35 x 35 x 192\n        x = self.Mixed_5b(x)\n        # 35 x 35 x 256\n        x = self.Mixed_5c(x)\n        # 35 x 35 x 288\n        x = self.Mixed_5d(x)\n        # 35 x 35 x 288\n\n        x = self.Mixed_6a(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6b(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6c(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6d(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6e(x)\n        # 17 x 17 x 768\n\n        # image region features\n        features = x\n        # 17 x 17 x 768\n\n        x = self.Mixed_7a(x)\n        # 8 x 8 x 1280\n        x = self.Mixed_7b(x)\n        # 8 x 8 x 2048\n        x = self.Mixed_7c(x)\n        # 8 x 8 x 2048\n        x = F.avg_pool2d(x, kernel_size=8)\n        # 1 x 1 x 2048\n        # x = F.dropout(x, training=self.training)\n        # 1 x 1 x 2048\n        x = x.view(x.size(0), -1)\n        # 2048\n\n        # global image features\n        cnn_code = self.emb_cnn_code(x)\n        # 512\n        if features is not None:\n            features = self.emb_features(features)\n        return features, cnn_code\n\n\n# ############## G networks ###################\nclass CA_NET(nn.Module):\n    # some code is modified from vae examples\n    # (https://github.com/pytorch/examples/blob/master/vae/main.py)\n    def __init__(self):\n        super(CA_NET, self).__init__()\n        self.t_dim = cfg.TEXT.EMBEDDING_DIM\n        self.c_dim = cfg.GAN.CONDITION_DIM\n        self.fc = nn.Linear(self.t_dim, self.c_dim * 4, bias=True)\n        self.relu = GLU()\n\n    def encode(self, text_embedding):\n        x = self.relu(self.fc(text_embedding))\n        mu = x[:, :self.c_dim]\n        logvar = x[:, self.c_dim:]\n        return mu, logvar\n\n    def reparametrize(self, mu, logvar):\n        std = logvar.mul(0.5).exp_()\n        if cfg.CUDA:\n            eps = torch.cuda.FloatTensor(std.size()).normal_()\n        else:\n            eps = torch.FloatTensor(std.size()).normal_()\n        eps = Variable(eps)\n        return eps.mul(std).add_(mu)\n\n    def forward(self, text_embedding):\n        mu, logvar = self.encode(text_embedding)\n        c_code = self.reparametrize(mu, logvar)\n        return c_code, mu, logvar\n\n\nclass INIT_STAGE_G(nn.Module):\n    def __init__(self, ngf, ncf):\n        super(INIT_STAGE_G, self).__init__()\n        self.gf_dim = ngf\n        self.in_dim = cfg.GAN.Z_DIM + ncf  # cfg.TEXT.EMBEDDING_DIM\n\n        self.define_module()\n\n    def define_module(self):\n        nz, ngf = self.in_dim, self.gf_dim\n        self.fc = nn.Sequential(\n            nn.Linear(nz, ngf * 4 * 4 * 2, bias=False),\n            nn.BatchNorm1d(ngf * 4 * 4 * 2),\n            GLU())\n\n        self.upsample1 = upBlock(ngf, ngf // 2)\n        self.upsample2 = upBlock(ngf // 2, ngf // 4)\n        self.upsample3 = upBlock(ngf // 4, ngf // 8)\n        self.upsample4 = upBlock(ngf // 8, ngf // 16)\n\n    def forward(self, z_code, c_code):\n        \"\"\"\n        :param z_code: batch x cfg.GAN.Z_DIM\n        :param c_code: batch x cfg.TEXT.EMBEDDING_DIM\n        :return: batch x ngf/16 x 64 x 64\n        \"\"\"\n        c_z_code = torch.cat((c_code, z_code), 1)\n        # state size ngf x 4 x 4\n        out_code = self.fc(c_z_code)\n        out_code = out_code.view(-1, self.gf_dim, 4, 4)\n        # state size ngf/3 x 8 x 8\n        out_code = self.upsample1(out_code)\n        # state size ngf/4 x 16 x 16\n        out_code = self.upsample2(out_code)\n        # state size ngf/8 x 32 x 32\n        out_code32 = self.upsample3(out_code)\n        # state size ngf/16 x 64 x 64\n        out_code64 = self.upsample4(out_code32)\n\n        return out_code64\n\n\nclass NEXT_STAGE_G(nn.Module):\n    def __init__(self, ngf, nef, ncf):\n        super(NEXT_STAGE_G, self).__init__()\n        self.gf_dim = ngf\n        self.ef_dim = nef\n        self.cf_dim = ncf\n        self.num_residual = cfg.GAN.R_NUM\n        self.define_module()\n\n    def _make_layer(self, block, channel_num):\n        layers = []\n        for i in range(cfg.GAN.R_NUM):\n            layers.append(block(channel_num))\n        return nn.Sequential(*layers)\n\n    def define_module(self):\n        ngf = self.gf_dim\n        self.att = ATT_NET(ngf, self.ef_dim)\n        self.residual = self._make_layer(ResBlock, ngf * 2)\n        self.upsample = upBlock(ngf * 2, ngf)\n\n    def forward(self, h_code, c_code, word_embs, mask):\n        \"\"\"\n            h_code1(query):  batch x idf x ih x iw (queryL=ihxiw)\n            word_embs(context): batch x cdf x sourceL (sourceL=seq_len)\n            c_code1: batch x idf x queryL\n            att1: batch x sourceL x queryL\n        \"\"\"\n        self.att.applyMask(mask)\n        c_code, att = self.att(h_code, word_embs)\n        h_c_code = torch.cat((h_code, c_code), 1)\n        out_code = self.residual(h_c_code)\n\n        # state size ngf/2 x 2in_size x 2in_size\n        out_code = self.upsample(out_code)\n\n        return out_code, att\n\n\nclass GET_IMAGE_G(nn.Module):\n    def __init__(self, ngf):\n        super(GET_IMAGE_G, self).__init__()\n        self.gf_dim = ngf\n        self.img = nn.Sequential(\n            conv3x3(ngf, 3),\n            nn.Tanh()\n        )\n\n    def forward(self, h_code):\n        out_img = self.img(h_code)\n        return out_img\n\n\nclass G_NET(nn.Module):\n    def __init__(self):\n        super(G_NET, self).__init__()\n        ngf = cfg.GAN.GF_DIM\n        nef = cfg.TEXT.EMBEDDING_DIM\n        ncf = cfg.GAN.CONDITION_DIM\n        self.ca_net = CA_NET()\n\n        if cfg.TREE.BRANCH_NUM > 0:\n            self.h_net1 = INIT_STAGE_G(ngf * 16, ncf)\n            self.img_net1 = GET_IMAGE_G(ngf)\n        # gf x 64 x 64\n        if cfg.TREE.BRANCH_NUM > 1:\n            self.h_net2 = NEXT_STAGE_G(ngf, nef, ncf)\n            self.img_net2 = GET_IMAGE_G(ngf)\n        if cfg.TREE.BRANCH_NUM > 2:\n            self.h_net3 = NEXT_STAGE_G(ngf, nef, ncf)\n            self.img_net3 = GET_IMAGE_G(ngf)\n\n    def forward(self, z_code, sent_emb, word_embs, mask):\n        \"\"\"\n            :param z_code: batch x cfg.GAN.Z_DIM\n            :param sent_emb: batch x cfg.TEXT.EMBEDDING_DIM\n            :param word_embs: batch x cdf x seq_len\n            :param mask: batch x seq_len\n            :return:\n        \"\"\"\n        fake_imgs = []\n        att_maps = []\n        c_code, mu, logvar = self.ca_net(sent_emb)\n\n        if cfg.TREE.BRANCH_NUM > 0:\n            h_code1 = self.h_net1(z_code, c_code)\n            fake_img1 = self.img_net1(h_code1)\n            fake_imgs.append(fake_img1)\n        if cfg.TREE.BRANCH_NUM > 1:\n            h_code2, att1 = \\\n                self.h_net2(h_code1, c_code, word_embs, mask)\n            fake_img2 = self.img_net2(h_code2)\n            fake_imgs.append(fake_img2)\n            if att1 is not None:\n                att_maps.append(att1)\n        if cfg.TREE.BRANCH_NUM > 2:\n            h_code3, att2 = \\\n                self.h_net3(h_code2, c_code, word_embs, mask)\n            fake_img3 = self.img_net3(h_code3)\n            fake_imgs.append(fake_img3)\n            if att2 is not None:\n                att_maps.append(att2)\n\n        return fake_imgs, att_maps, mu, logvar\n\n\n\nclass G_DCGAN(nn.Module):\n    def __init__(self):\n        super(G_DCGAN, self).__init__()\n        ngf = cfg.GAN.GF_DIM\n        nef = cfg.TEXT.EMBEDDING_DIM\n        ncf = cfg.GAN.CONDITION_DIM\n        self.ca_net = CA_NET()\n\n        # 16gf x 64 x 64 --> gf x 64 x 64 --> 3 x 64 x 64\n        if cfg.TREE.BRANCH_NUM > 0:\n            self.h_net1 = INIT_STAGE_G(ngf * 16, ncf)\n        # gf x 64 x 64\n        if cfg.TREE.BRANCH_NUM > 1:\n            self.h_net2 = NEXT_STAGE_G(ngf, nef, ncf)\n        if cfg.TREE.BRANCH_NUM > 2:\n            self.h_net3 = NEXT_STAGE_G(ngf, nef, ncf)\n        self.img_net = GET_IMAGE_G(ngf)\n\n    def forward(self, z_code, sent_emb, word_embs, mask):\n        \"\"\"\n            :param z_code: batch x cfg.GAN.Z_DIM\n            :param sent_emb: batch x cfg.TEXT.EMBEDDING_DIM\n            :param word_embs: batch x cdf x seq_len\n            :param mask: batch x seq_len\n            :return:\n        \"\"\"\n        att_maps = []\n        c_code, mu, logvar = self.ca_net(sent_emb)\n        if cfg.TREE.BRANCH_NUM > 0:\n            h_code = self.h_net1(z_code, c_code)\n        if cfg.TREE.BRANCH_NUM > 1:\n            h_code, att1 = self.h_net2(h_code, c_code, word_embs, mask)\n            if att1 is not None:\n                att_maps.append(att1)\n        if cfg.TREE.BRANCH_NUM > 2:\n            h_code, att2 = self.h_net3(h_code, c_code, word_embs, mask)\n            if att2 is not None:\n                att_maps.append(att2)\n\n        fake_imgs = self.img_net(h_code)\n        return [fake_imgs], att_maps, mu, logvar\n\n\n# ############## D networks ##########################\ndef Block3x3_leakRelu(in_planes, out_planes):\n    block = nn.Sequential(\n        conv3x3(in_planes, out_planes),\n        nn.BatchNorm2d(out_planes),\n        nn.LeakyReLU(0.2, inplace=True)\n    )\n    return block\n\n\n# Downsale the spatial size by a factor of 2\ndef downBlock(in_planes, out_planes):\n    block = nn.Sequential(\n        nn.Conv2d(in_planes, out_planes, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(out_planes),\n        nn.LeakyReLU(0.2, inplace=True)\n    )\n    return block\n\n\n# Downsale the spatial size by a factor of 16\ndef encode_image_by_16times(ndf):\n    encode_img = nn.Sequential(\n        # --> state size. ndf x in_size/2 x in_size/2\n        nn.Conv2d(3, ndf, 4, 2, 1, bias=False),\n        nn.LeakyReLU(0.2, inplace=True),\n        # --> state size 2ndf x x in_size/4 x in_size/4\n        nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ndf * 2),\n        nn.LeakyReLU(0.2, inplace=True),\n        # --> state size 4ndf x in_size/8 x in_size/8\n        nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ndf * 4),\n        nn.LeakyReLU(0.2, inplace=True),\n        # --> state size 8ndf x in_size/16 x in_size/16\n        nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ndf * 8),\n        nn.LeakyReLU(0.2, inplace=True)\n    )\n    return encode_img\n\n\nclass D_GET_LOGITS(nn.Module):\n    def __init__(self, ndf, nef, bcondition=False):\n        super(D_GET_LOGITS, self).__init__()\n        self.df_dim = ndf\n        self.ef_dim = nef\n        self.bcondition = bcondition\n        if self.bcondition:\n            self.jointConv = Block3x3_leakRelu(ndf * 8 + nef, ndf * 8)\n\n        self.outlogits = nn.Sequential(\n            nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=4),\n            nn.Sigmoid())\n\n    def forward(self, h_code, c_code=None):\n        if self.bcondition and c_code is not None:\n            # conditioning output\n            c_code = c_code.view(-1, self.ef_dim, 1, 1)\n            c_code = c_code.repeat(1, 1, 4, 4)\n            # state size (ngf+egf) x 4 x 4\n            h_c_code = torch.cat((h_code, c_code), 1)\n            # state size ngf x in_size x in_size\n            h_c_code = self.jointConv(h_c_code)\n        else:\n            h_c_code = h_code\n\n        output = self.outlogits(h_c_code)\n        return output.view(-1)\n\n\n# For 64 x 64 images\nclass D_NET64(nn.Module):\n    def __init__(self, b_jcu=True):\n        super(D_NET64, self).__init__()\n        ndf = cfg.GAN.DF_DIM\n        nef = cfg.TEXT.EMBEDDING_DIM\n        self.img_code_s16 = encode_image_by_16times(ndf)\n        if b_jcu:\n            self.UNCOND_DNET = D_GET_LOGITS(ndf, nef, bcondition=False)\n        else:\n            self.UNCOND_DNET = None\n        self.COND_DNET = D_GET_LOGITS(ndf, nef, bcondition=True)\n\n    def forward(self, x_var):\n        x_code4 = self.img_code_s16(x_var)  # 4 x 4 x 8df\n        return x_code4\n\n\n# For 128 x 128 images\nclass D_NET128(nn.Module):\n    def __init__(self, b_jcu=True):\n        super(D_NET128, self).__init__()\n        ndf = cfg.GAN.DF_DIM\n        nef = cfg.TEXT.EMBEDDING_DIM\n        self.img_code_s16 = encode_image_by_16times(ndf)\n        self.img_code_s32 = downBlock(ndf * 8, ndf * 16)\n        self.img_code_s32_1 = Block3x3_leakRelu(ndf * 16, ndf * 8)\n        #\n        if b_jcu:\n            self.UNCOND_DNET = D_GET_LOGITS(ndf, nef, bcondition=False)\n        else:\n            self.UNCOND_DNET = None\n        self.COND_DNET = D_GET_LOGITS(ndf, nef, bcondition=True)\n\n    def forward(self, x_var):\n        x_code8 = self.img_code_s16(x_var)   # 8 x 8 x 8df\n        x_code4 = self.img_code_s32(x_code8)   # 4 x 4 x 16df\n        x_code4 = self.img_code_s32_1(x_code4)  # 4 x 4 x 8df\n        return x_code4\n\n\n# For 256 x 256 images\nclass D_NET256(nn.Module):\n    def __init__(self, b_jcu=True):\n        super(D_NET256, self).__init__()\n        ndf = cfg.GAN.DF_DIM\n        nef = cfg.TEXT.EMBEDDING_DIM\n        self.img_code_s16 = encode_image_by_16times(ndf)\n        self.img_code_s32 = downBlock(ndf * 8, ndf * 16)\n        self.img_code_s64 = downBlock(ndf * 16, ndf * 32)\n        self.img_code_s64_1 = Block3x3_leakRelu(ndf * 32, ndf * 16)\n        self.img_code_s64_2 = Block3x3_leakRelu(ndf * 16, ndf * 8)\n        if b_jcu:\n            self.UNCOND_DNET = D_GET_LOGITS(ndf, nef, bcondition=False)\n        else:\n            self.UNCOND_DNET = None\n        self.COND_DNET = D_GET_LOGITS(ndf, nef, bcondition=True)\n\n    def forward(self, x_var):\n        x_code16 = self.img_code_s16(x_var)\n        x_code8 = self.img_code_s32(x_code16)\n        x_code4 = self.img_code_s64(x_code8)\n        x_code4 = self.img_code_s64_1(x_code4)\n        x_code4 = self.img_code_s64_2(x_code4)\n        return x_code4","metadata":{"execution":{"iopub.status.busy":"2023-05-11T09:06:36.204072Z","iopub.execute_input":"2023-05-11T09:06:36.204471Z","iopub.status.idle":"2023-05-11T09:06:36.230501Z","shell.execute_reply.started":"2023-05-11T09:06:36.204421Z","shell.execute_reply":"2023-05-11T09:06:36.229255Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Overwriting code/model.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile code/pretrain_DAMSM.py\nfrom __future__ import print_function\n\nfrom miscc.utils import mkdir_p\nfrom miscc.utils import build_super_images\nfrom miscc.losses import sent_loss, words_loss\nfrom miscc.config import cfg, cfg_from_file\n\nfrom model import TRANSFORMER_ENCODER, RNN_ENCODER, CNN_ENCODER\n\nimport os\nimport sys\nimport time\nimport random\nimport pprint\nimport datetime\nimport dateutil.tz\nimport argparse\nimport numpy as np\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as transforms\n# from torch.nn.utils.rnn import pad_packed_sequence\n\nfrom transformers import AlbertModel\n\n\ndir_path = (os.path.abspath(os.path.join(os.path.realpath(__file__), './.')))\nsys.path.append(dir_path)\n\n\nUPDATE_INTERVAL = 50\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Train a DAMSM network')\n    parser.add_argument('--cfg', dest='cfg_file',\n                        help='optional config file',\n                        default='cfg/DAMSM/bird.yml', type=str)\n    parser.add_argument('--gpu', dest='gpu_id', type=int, default=0)\n    parser.add_argument('--data_dir', dest='data_dir', type=str, default='')\n    parser.add_argument('--manualSeed', type=int, help='manual seed')\n    args = parser.parse_args()\n    return args\n\n\ndef train( dataloader, cnn_model, nlp_model, batch_size,\n           labels, optimizer, epoch, ixtoword, image_dir ):\n    \n    if cfg.DATASET_NAME=='birds':\n        from datasets import TextDataset\n        from datasets import prepare_data\n    if cfg.DATASET_NAME=='flowers':\n        from datasets_flowers import TextDataset\n        from datasets_flowers import prepare_data\n        \n    cnn_model.train()\n    nlp_model.train()\n    s_total_loss0 = 0\n    s_total_loss1 = 0\n    w_total_loss0 = 0\n    w_total_loss1 = 0\n    count = (epoch + 1) * len(dataloader)\n    start_time = time.time()\n    for step, data in enumerate(dataloader, 0):\n        # print('step', step)\n        nlp_model.zero_grad()\n        cnn_model.zero_grad()\n\n        imgs, imgs_2, captions, cap_lens, class_ids, keys, captions_2, cap_lens_2, class_ids_2, \\\n        sort_ind, sort_ind_2 = prepare_data(data)\n\n\n        # words_features: batch_size x nef x 17 x 17\n        # sent_code: batch_size x nef\n        words_features, sent_code = cnn_model(imgs[-1])\n        # print( words_features.shape, sent_code.shape )\n        # --> batch_size x nef x 17*17\n        nef, att_sze = words_features.size(1), words_features.size(2)\n        # words_features = words_features.view(batch_size, nef, -1)\n\n        # Forward Prop:\n        # inputs:\n        #   captions: torch.LongTensor of ids of size batch x n_steps\n        # outputs:\n        #   words_emb: batch_size x nef x seq_len\n        #   sent_emb: batch_size x nef\n        \n        words_emb = nlp_model( captions )[0].transpose(1, 2).contiguous()\n        sent_emb = words_emb[ :, :, -1 ].contiguous()\n            # sent_emb = sent_emb.view(batch_size, -1)\n        # print( words_emb.shape, sent_emb.shape )\n\n        # Compute Loss:\n        # NOTE: the ideal loss for Transformer may be different than that for bi-directional LSTM\n        w_loss0, w_loss1, attn_maps = words_loss( words_features, words_emb, labels,\n                                                  cap_lens, class_ids, batch_size )\n        w_total_loss0 += w_loss0.data\n        w_total_loss1 += w_loss1.data\n        loss = w_loss0 + w_loss1\n\n        s_loss0, s_loss1 = \\\n            sent_loss( sent_code, sent_emb, labels, class_ids, batch_size )\n        loss += s_loss0 + s_loss1\n        s_total_loss0 += s_loss0.data\n        s_total_loss1 += s_loss1.data\n        #\n        # Backprop:\n        loss.backward()\n        #\n        # `clip_grad_norm` helps prevent\n        # the exploding gradient problem in RNNs / LSTMs.\n        optimizer.step()\n\n        if step % UPDATE_INTERVAL == 0:\n            count = epoch * len(dataloader) + step\n\n            # print(  s_total_loss0, s_total_loss1 )\n            s_cur_loss0 = s_total_loss0.item() / UPDATE_INTERVAL\n            s_cur_loss1 = s_total_loss1.item() / UPDATE_INTERVAL\n\n            # print(  w_total_loss0, w_total_loss1 )\n            w_cur_loss0 = w_total_loss0.item() / UPDATE_INTERVAL\n            w_cur_loss1 = w_total_loss1.item() / UPDATE_INTERVAL\n\n            elapsed = time.time() - start_time\n            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n                  's_loss {:5.5f} {:5.5f} | '\n                  'w_loss {:5.5f} {:5.5f}'\n                  .format(epoch, step, len(dataloader),\n                          elapsed * 1000. / UPDATE_INTERVAL,\n                          s_cur_loss0, s_cur_loss1,\n                          w_cur_loss0, w_cur_loss1))\n            s_total_loss0 = 0\n            s_total_loss1 = 0\n            w_total_loss0 = 0\n            w_total_loss1 = 0\n            start_time = time.time()\n\n            # Attention Maps\n            img_set, _ = \\\n                build_super_images(imgs[-1].cpu(), captions,\n                                   ixtoword, attn_maps, att_sze)\n            if img_set is not None:\n                im = Image.fromarray(img_set)\n                fullpath = '%s/attention_maps%d.png' % (image_dir, step)\n                im.save(fullpath)\n    return count\n\n\ndef evaluate(dataloader, cnn_model, nlp_model, batch_size):\n    cnn_model.eval()\n    nlp_model.eval()    \n    s_total_loss = 0\n    w_total_loss = 0\n    if cfg.DATASET_NAME=='birds':\n        from datasets import TextDataset\n        from datasets import prepare_data\n    if cfg.DATASET_NAME=='flowers':\n        from datasets_flowers import TextDataset\n        from datasets_flowers import prepare_data\n    \n    for step, data in enumerate(dataloader, 0):\n        real_imgs, imgs_2, captions, cap_lens, class_ids, keys, captions_2, cap_lens_2, class_ids_2, \\\n        sort_ind, sort_ind_2 = prepare_data(data)\n\n        words_features, sent_code = cnn_model(real_imgs[-1])\n        # nef = words_features.size(1)\n        # words_features = words_features.view(batch_size, nef, -1)\n\n        words_emb = nlp_model( captions )[0].transpose(1, 2).contiguous()\n        sent_emb = words_emb[ :, :, -1 ].contiguous()\n            # sent_emb = sent_emb.view(batch_size, -1)\n\n        w_loss0, w_loss1, attn = words_loss( words_features, words_emb, labels,\n                                             cap_lens, class_ids, batch_size )\n        w_total_loss += ( w_loss0 + w_loss1 ).data\n\n        s_loss0, s_loss1 = \\\n            sent_loss( sent_code, sent_emb, labels, class_ids, batch_size )\n        s_total_loss += ( s_loss0 + s_loss1 ).data\n\n        if step == 50:\n            break\n\n    s_cur_loss = s_total_loss.item() / step\n    w_cur_loss = w_total_loss.item() / step\n\n    return s_cur_loss, w_cur_loss\n\n\ndef build_models():\n    # build model ############################################################\n    text_encoder = AlbertModel.from_pretrained('albert-base-v2')\n\n    image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)\n\n    labels = Variable(torch.LongTensor(range(batch_size)))\n    start_epoch = 0\n    if cfg.TRAIN.NET_E:\n        state_dict = torch.load(cfg.TRAIN.NET_E)\n        text_encoder.load_state_dict(state_dict)\n        print('Load ', cfg.TRAIN.NET_E)\n                                                      # output_hidden_states = True )\n          #\n        name = cfg.TRAIN.NET_E.replace( 'text_encoder', 'image_encoder' )\n        state_dict = torch.load(name)\n        image_encoder.load_state_dict(state_dict)\n        print('Load ', name)\n\n        istart = cfg.TRAIN.NET_E.rfind('_') + 8\n        iend = cfg.TRAIN.NET_E.rfind('.')\n        start_epoch = cfg.TRAIN.NET_E[istart:iend]\n        start_epoch = int(start_epoch) + 1\n\n    if cfg.CUDA:\n        text_encoder = text_encoder.cuda()\n        image_encoder = image_encoder.cuda()\n        labels = labels.cuda()\n\n    return text_encoder, image_encoder, labels, start_epoch\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    if args.cfg_file is not None:\n        cfg_from_file(args.cfg_file)\n\n    if args.gpu_id == -1:\n        cfg.CUDA = False\n    else:\n        cfg.GPU_ID = args.gpu_id\n\n    if args.data_dir != '':\n        cfg.DATA_DIR = args.data_dir\n    print('Using config:')\n    pprint.pprint(cfg)\n\n    if not cfg.TRAIN.FLAG:\n        args.manualSeed = 100\n    elif args.manualSeed is None:\n        args.manualSeed = random.randint(1, 10000)\n    random.seed(args.manualSeed)\n    np.random.seed(args.manualSeed)\n    torch.manual_seed(args.manualSeed)\n    if cfg.CUDA:\n        torch.cuda.manual_seed_all(args.manualSeed)\n    if cfg.DATASET_NAME=='birds':\n        from datasets import TextDataset\n        from datasets import prepare_data\n    if cfg.DATASET_NAME=='flowers':\n        from datasets_flowers import TextDataset\n        from datasets_flowers import prepare_data\n\n    ##########################################################################\n    now = datetime.datetime.now(dateutil.tz.tzlocal())\n    timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n    output_dir = 'damsm_output/%s_%s_%s' % \\\n        (cfg.DATASET_NAME, cfg.CONFIG_NAME, timestamp)\n\n    model_dir = os.path.join(output_dir, 'Model')\n    image_dir = os.path.join(output_dir, 'Image')\n    mkdir_p(model_dir)\n    mkdir_p(image_dir)\n\n    torch.cuda.set_device(cfg.GPU_ID)\n    cudnn.benchmark = True\n\n    # Get data loader ##################################################\n    imsize = cfg.TREE.BASE_SIZE * (2 ** (cfg.TREE.BRANCH_NUM-1))\n    batch_size = cfg.TRAIN.BATCH_SIZE\n    image_transform = transforms.Compose([\n        transforms.Resize(int(imsize * 76 / 64)),\n        transforms.RandomCrop(imsize),\n        transforms.RandomHorizontalFlip()])\n    dataset = TextDataset(cfg.DATA_DIR, 'train',\n                          base_size=cfg.TREE.BASE_SIZE,\n                          transform=image_transform)\n\n    print(dataset.n_words, dataset.embeddings_num)\n    assert dataset\n    dataloader = torch.utils.data.DataLoader(\n        dataset, batch_size=batch_size, drop_last=True,\n        shuffle=True, num_workers=int(cfg.WORKERS))\n\n    # # validation data #\n    dataset_val = TextDataset(cfg.DATA_DIR, 'test',\n                              base_size=cfg.TREE.BASE_SIZE,\n                              transform=image_transform)\n    dataloader_val = torch.utils.data.DataLoader(\n        dataset_val, batch_size=batch_size, drop_last=True,\n        shuffle=True, num_workers=int(cfg.WORKERS))\n\n    # Train ##############################################################\n    text_encoder, image_encoder, labels, start_epoch = build_models()\n    para = list(text_encoder.parameters())\n    for v in image_encoder.parameters():\n        if v.requires_grad:\n            para.append(v)\n    # optimizer = optim.Adam(para, lr=cfg.TRAIN.ENCODER_LR, betas=(0.5, 0.999))\n    # At any point you can hit Ctrl + C to break out of training early.\n    try:\n        lr = cfg.TRAIN.ENCODER_LR\n        for epoch in range(start_epoch, cfg.TRAIN.MAX_EPOCH):\n            optimizer = optim.Adam(para, lr=lr, betas=(0.5, 0.999))\n            epoch_start_time = time.time()\n            count = train(dataloader, image_encoder, text_encoder,\n                          batch_size, labels, optimizer, epoch,\n                          dataset.ixtoword, image_dir)\n            print('-' * 89)\n            if len(dataloader_val) > 0:\n                s_loss, w_loss = evaluate(dataloader_val, image_encoder,\n                                          text_encoder, batch_size)\n                print('| end epoch {:3d} | valid loss '\n                      '{:5.5f} {:5.5f} | lr {:.8f}|'\n                      .format(epoch, s_loss, w_loss, lr))\n            print('-' * 89)\n            if lr > cfg.TRAIN.ENCODER_LR/10.:\n                lr *= 0.98\n\n            if (epoch % cfg.TRAIN.SNAPSHOT_INTERVAL == 0 or\n                epoch == cfg.TRAIN.MAX_EPOCH):\n                torch.save(image_encoder.state_dict(),\n                           '%s/image_encoder%d.pth' % (model_dir, epoch))\n                torch.save(text_encoder.state_dict(),\n                           '%s/text_encoder%d.pth' % (model_dir, epoch))\n                print('Save G/Ds models.')\n    except KeyboardInterrupt:\n        print('-' * 89)\n        print('Exiting from training early')","metadata":{"execution":{"iopub.status.busy":"2023-05-11T09:10:03.149715Z","iopub.execute_input":"2023-05-11T09:10:03.150052Z","iopub.status.idle":"2023-05-11T09:10:03.166307Z","shell.execute_reply.started":"2023-05-11T09:10:03.150024Z","shell.execute_reply":"2023-05-11T09:10:03.165279Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Overwriting code/pretrain_DAMSM.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile code/trainer.py\nfrom __future__ import print_function\nfrom six.moves import range\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\n\nfrom PIL import Image\n\nfrom miscc.config import cfg\nfrom miscc.utils import mkdir_p\nfrom miscc.utils import build_super_images, build_super_images2\nfrom miscc.utils import weights_init, load_params, copy_G_params\nfrom model import G_DCGAN, G_NET\nfrom model import RNN_ENCODER, CNN_ENCODER\n\nfrom miscc.losses import words_loss\nfrom miscc.losses import discriminator_loss, generator_loss, KL_loss\nimport os\nimport time\nimport numpy as np\nimport sys\n\nfrom masks import mask_correlated_samples\nfrom nt_xent import NT_Xent\nfrom transformers import AlbertModel\n\nfrom torch.utils.tensorboard import SummaryWriter\n\ndef l2norm(X, dim, eps=1e-8):\n    \"\"\"L2-normalize columns of X\n    \"\"\"\n    norm = torch.pow(X, 2).sum(dim=dim, keepdim=True).sqrt() + eps\n    X = torch.div(X, norm)\n    return X\n\n\n# ################# Text to image task############################ #\nclass condGANTrainer(object):\n    def __init__(self, output_dir, data_loader, n_words, ixtoword, dataset):\n        if cfg.TRAIN.FLAG:\n            self.model_dir = os.path.join(output_dir, 'Model')\n            self.image_dir = os.path.join(output_dir, 'Image')\n            mkdir_p(self.model_dir)\n            mkdir_p(self.image_dir)\n\n        torch.cuda.set_device(cfg.GPU_ID)\n        cudnn.benchmark = True\n\n        self.batch_size = cfg.TRAIN.BATCH_SIZE\n        self.max_epoch = cfg.TRAIN.MAX_EPOCH\n        self.snapshot_interval = cfg.TRAIN.SNAPSHOT_INTERVAL\n\n        self.n_words = n_words\n        self.ixtoword = ixtoword\n        self.data_loader = data_loader\n        self.num_batches = len(self.data_loader)\n        self.dataset = dataset\n        self.writer = SummaryWriter('runs/visualize')\n\n\n    def build_models(self):\n        # ###################encoders######################################## #\n        if cfg.TRAIN.NET_E == '':\n            print('Error: no pretrained text-image encoders')\n            return\n\n        image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)\n        img_encoder_path = cfg.TRAIN.NET_E.replace('text_encoder', 'image_encoder')\n        state_dict = \\\n            torch.load(img_encoder_path, map_location=lambda storage, loc: storage)\n        image_encoder.load_state_dict(state_dict)\n        for p in image_encoder.parameters():\n            p.requires_grad = False\n        print('Load image encoder from:', img_encoder_path)\n        image_encoder.eval()\n\n        text_encoder = AlbertModel.from_pretrained(\"albert-base-v2\")\n        state_dict = \\\n            torch.load(cfg.TRAIN.NET_E,\n                       map_location=lambda storage, loc: storage)\n        text_encoder.load_state_dict(state_dict)\n        for p in text_encoder.parameters():\n            p.requires_grad = False\n        print('Load text encoder from:', cfg.TRAIN.NET_E)\n        text_encoder.eval()\n\n        # #######################generator and discriminators############## #\n        netsD = []\n        if cfg.GAN.B_DCGAN:\n            if cfg.TREE.BRANCH_NUM ==1:\n                from model import D_NET64 as D_NET\n            elif cfg.TREE.BRANCH_NUM == 2:\n                from model import D_NET128 as D_NET\n            else:  # cfg.TREE.BRANCH_NUM == 3:\n                from model import D_NET256 as D_NET\n            # TODO: elif cfg.TREE.BRANCH_NUM > 3:\n            netG = G_DCGAN()\n            netsD = [D_NET(b_jcu=False)]\n        else:\n            from model import D_NET64, D_NET128, D_NET256\n            netG = G_NET()\n            if cfg.TREE.BRANCH_NUM > 0:\n                netsD.append(D_NET64())\n            if cfg.TREE.BRANCH_NUM > 1:\n                netsD.append(D_NET128())\n            if cfg.TREE.BRANCH_NUM > 2:\n                netsD.append(D_NET256())\n            # TODO: if cfg.TREE.BRANCH_NUM > 3:\n        netG.apply(weights_init)\n        # print(netG)\n        for i in range(len(netsD)):\n            netsD[i].apply(weights_init)\n            # print(netsD[i])\n        print('# of netsD', len(netsD))\n        #\n        epoch = 0\n        if cfg.TRAIN.NET_G != '':\n            state_dict = \\\n                torch.load(cfg.TRAIN.NET_G, map_location=lambda storage, loc: storage)\n            netG.load_state_dict(state_dict)\n            print('Load G from: ', cfg.TRAIN.NET_G)\n            istart = cfg.TRAIN.NET_G.rfind('_') + 1\n            iend = cfg.TRAIN.NET_G.rfind('.')\n            epoch = cfg.TRAIN.NET_G[istart:iend]\n            epoch = int(epoch) + 1\n            if cfg.TRAIN.B_NET_D:\n                Gname = cfg.TRAIN.NET_G\n                for i in range(len(netsD)):\n                    s_tmp = Gname[:Gname.rfind('/')]\n                    Dname = '%s/netD%d.pth' % (s_tmp, i)\n                    print('Load D from: ', Dname)\n                    state_dict = \\\n                        torch.load(Dname, map_location=lambda storage, loc: storage)\n                    netsD[i].load_state_dict(state_dict)\n        # ########################################################### #\n        if cfg.CUDA:\n            text_encoder = text_encoder.cuda()\n            image_encoder = image_encoder.cuda()\n            netG.cuda()\n            for i in range(len(netsD)):\n                netsD[i].cuda()\n        return [text_encoder, image_encoder, netG, netsD, epoch]\n\n    def define_optimizers(self, netG, netsD):\n        optimizersD = []\n        num_Ds = len(netsD)\n        for i in range(num_Ds):\n            opt = optim.Adam(netsD[i].parameters(),\n                             lr=cfg.TRAIN.DISCRIMINATOR_LR,\n                             betas=(0.5, 0.999))\n            optimizersD.append(opt)\n\n        optimizerG = optim.Adam(netG.parameters(),\n                                lr=cfg.TRAIN.GENERATOR_LR,\n                                betas=(0.5, 0.999))\n\n        return optimizerG, optimizersD\n\n    def prepare_labels(self):\n        batch_size = self.batch_size\n        real_labels = Variable(torch.FloatTensor(batch_size).fill_(1))\n        fake_labels = Variable(torch.FloatTensor(batch_size).fill_(0))\n        match_labels = Variable(torch.LongTensor(range(batch_size)))\n        if cfg.CUDA:\n            real_labels = real_labels.cuda()\n            fake_labels = fake_labels.cuda()\n            match_labels = match_labels.cuda()\n\n        return real_labels, fake_labels, match_labels\n\n    def save_model(self, netG, avg_param_G, netsD, epoch):\n        backup_para = copy_G_params(netG)\n        load_params(netG, avg_param_G)\n        torch.save(netG.state_dict(),\n            '%s/netG_epoch_%d.pth' % (self.model_dir, epoch))\n        load_params(netG, backup_para)\n        #\n        for i in range(len(netsD)):\n            netD = netsD[i]\n            torch.save(netD.state_dict(),\n                '%s/netD%d.pth' % (self.model_dir, i))\n        print('Save G/Ds models.')\n\n    def set_requires_grad_value(self, models_list, brequires):\n        for i in range(len(models_list)):\n            for p in models_list[i].parameters():\n                p.requires_grad = brequires\n\n    def save_img_results(self, netG, noise, sent_emb, words_embs, mask,\n                         image_encoder, captions, cap_lens,\n                         gen_iterations, name='current'):\n        # Save images\n        fake_imgs, attention_maps, _, _ = netG(noise, sent_emb, words_embs, mask)\n        for i in range(len(attention_maps)):\n            if len(fake_imgs) > 1:\n                img = fake_imgs[i + 1].detach().cpu()\n                lr_img = fake_imgs[i].detach().cpu()\n            else:\n                img = fake_imgs[0].detach().cpu()\n                lr_img = None\n            attn_maps = attention_maps[i]\n            att_sze = attn_maps.size(2)\n            img_set, _ = \\\n                build_super_images(img, captions, self.ixtoword,\n                                   attn_maps, att_sze, lr_imgs=lr_img)\n            if img_set is not None:\n                im = Image.fromarray(img_set)\n                fullpath = '%s/G_%s_%d_%d.png'\\\n                    % (self.image_dir, name, gen_iterations, i)\n                im.save(fullpath)\n\n        # for i in range(len(netsD)):\n        i = -1\n        img = fake_imgs[i].detach()\n        region_features, _ = image_encoder(img)\n        att_sze = region_features.size(2)\n        _, _, att_maps = words_loss(region_features.detach(),\n                                    words_embs.detach(),\n                                    None, cap_lens,\n                                    None, self.batch_size)\n        img_set, _ = \\\n            build_super_images(fake_imgs[i].detach().cpu(),\n                               captions, self.ixtoword, att_maps, att_sze)\n        if img_set is not None:\n            im = Image.fromarray(img_set)\n            fullpath = '%s/D_%s_%d.png'\\\n                % (self.image_dir, name, gen_iterations)\n            im.save(fullpath)\n\n    def train(self):\n        text_encoder, image_encoder, netG, netsD, start_epoch = self.build_models()\n        avg_param_G = copy_G_params(netG)\n        optimizerG, optimizersD = self.define_optimizers(netG, netsD)\n        real_labels, fake_labels, match_labels = self.prepare_labels()\n\n        real_labels_2, fake_labels_2, match_labels_2 = self.prepare_labels()\n\n        batch_size = self.batch_size\n        nz = cfg.GAN.Z_DIM\n        noise = Variable(torch.FloatTensor(batch_size, nz))\n        fixed_noise = Variable(torch.FloatTensor(batch_size, nz).normal_(0, 1))\n        if cfg.CUDA:\n            noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n\n        gen_iterations = 0\n\n        mask = mask_correlated_samples(self)\n\n        temperature = 0.5\n        device = noise.get_device()\n        criterion = NT_Xent(batch_size, temperature, mask, device)\n        if cfg.DATASET_NAME == 'birds':\n            from datasets import prepare_data\n        if cfg.DATASET_NAME == 'flowers':\n            from datasets_flowers import prepare_data\n\n        # gen_iterations = start_epoch * self.num_batches\n        for epoch in range(start_epoch, self.max_epoch):\n            start_t = time.time()\n\n            data_iter = iter(self.data_loader)\n            step = 0\n\n            D_total_loss = 0\n            G_total_loss = 0\n\n            while step < self.num_batches:\n                # reset requires_grad to be trainable for all Ds\n                # self.set_requires_grad_value(netsD, True)\n\n                ######################################################\n                # (1) Prepare training data and Compute text embeddings\n                ######################################################\n                data = next(data_iter)\n                imgs, imgs_2, captions, cap_lens, class_ids, keys, captions_2, cap_lens_2, class_ids_2, \\\n                sort_ind, sort_ind_2 = prepare_data(data)\n\n                words_embs = text_encoder( captions )[0].transpose(1, 2).contiguous()\n                    #words_embs = torch.Tensor(words_embs).cuda()\n                sent_emb = words_embs[ :, :, -1 ].contiguous()\n                words_embs, sent_emb = words_embs.detach(), sent_emb.detach()\n                mask = (captions == 0)\n                num_words = words_embs.size(2)\n                if mask.size(1) > num_words:\n                    mask = mask[:, :num_words]\n\n                words_embs_2 = text_encoder( captions_2 )[0].transpose(1, 2).contiguous()\n                    #words_embs = torch.Tensor(words_embs).cuda()\n                sent_emb_2 = words_embs_2[ :, :, -1 ].contiguous()\n                words_embs_2, sent_emb_2 = words_embs_2.detach(), sent_emb_2.detach()\n                mask_2 = (captions_2 == 0)\n                num_words_2 = words_embs_2.size(2)\n                if mask_2.size(1) > num_words_2:\n                    mask_2 = mask_2[:, :num_words_2]\n\n                #######################################################\n                # (2) Generate fake images\n                ######################################################\n                noise.data.normal_(0, 1)\n                fake_imgs, _, mu, logvar = netG(noise, sent_emb, words_embs, mask)\n                fake_imgs_2, _, mu_2, logvar_2 = netG(noise, sent_emb_2, words_embs_2, mask_2)\n\n                #######################################################\n                # (3) Update D network\n                ######################################################\n                errD_total = 0\n                D_logs = ''\n                for i in range(len(netsD)):\n                    netsD[i].zero_grad()\n                    errD = discriminator_loss(netsD[i], imgs[i], fake_imgs[i],\n                                              sent_emb, real_labels, fake_labels)\n                    errD_2 = discriminator_loss(netsD[i], imgs_2[i], fake_imgs_2[i],\n                                                sent_emb_2, real_labels_2, fake_labels_2)\n                    errD += errD_2\n\n                    # backward and update parameters\n                    errD.backward()\n                    optimizersD[i].step()\n                    errD_total += errD\n                    D_logs += 'errD%d: %.2f ' % (i, errD.item())\n\n                #######################################################\n                # (4) Update G network: maximize log(D(G(z)))\n                ######################################################\n                # compute total loss for training G\n                step += 1\n                gen_iterations += 1\n\n                # do not need to compute gradient for Ds\n                # self.set_requires_grad_value(netsD, False)\n                netG.zero_grad()\n                errG_total, G_logs, cnn_code = \\\n                    generator_loss(netsD, image_encoder, fake_imgs, real_labels,\n                                   words_embs, sent_emb, match_labels, cap_lens, class_ids)\n                kl_loss = KL_loss(mu, logvar)\n                errG_total += kl_loss\n                G_logs += 'kl_loss: %.2f ' % kl_loss.item()\n\n                errG_total_2, G_logs_2, cnn_code_2 = \\\n                    generator_loss(netsD, image_encoder, fake_imgs_2, real_labels_2,\n                                   words_embs_2, sent_emb_2, match_labels_2, cap_lens_2, class_ids_2)\n                kl_loss_2 = KL_loss(mu_2, logvar_2)\n                errG_total_2 += kl_loss_2\n                G_logs_2 += 'kl_loss: %.2f ' % kl_loss_2.item()\n\n                errG_total += errG_total_2\n\n                _, ori_indices = torch.sort(sort_ind, 0)\n                _, ori_indices_2 = torch.sort(sort_ind_2, 0)\n\n                total_contra_loss = 0\n                i = -1\n                cnn_code = cnn_code[ori_indices]\n                cnn_code_2 = cnn_code_2[ori_indices_2]\n\n                cnn_code = l2norm(cnn_code, dim=1)\n                cnn_code_2 = l2norm(cnn_code_2, dim=1)\n\n                contrative_loss = criterion(cnn_code, cnn_code_2)\n                total_contra_loss += contrative_loss *  0.2\n                G_logs += 'contrative_loss: %.2f ' % total_contra_loss.item()\n                errG_total += total_contra_loss\n                # backward and update parameters\n                errG_total.backward()\n                optimizerG.step()\n                for p, avg_p in zip(netG.parameters(), avg_param_G):\n                    avg_p.mul_(0.999).add_(0.001, p.data)\n\n                if gen_iterations % 100 == 0:\n                    print(D_logs + '\\n' + G_logs + '\\n' + G_logs_2)\n                # save images\n                if gen_iterations % 1000 == 0:\n                    backup_para = copy_G_params(netG)\n                    load_params(netG, avg_param_G)\n                    self.save_img_results(netG, fixed_noise, sent_emb,\n                                           words_embs, mask, image_encoder,\n                                           captions, cap_lens, epoch, name='average')\n                    load_params(netG, backup_para)\n                    #\n                    self.save_img_results(netG, fixed_noise, sent_emb,\n                                           words_embs, mask, image_encoder,\n                                           captions, cap_lens,\n                                           epoch, name='current')\n                D_total_loss += errD_total.item()\n                G_total_loss += errG_total.item()\n\n            end_t = time.time()\n\n            print('''[%d/%d][%d]\n                  Loss_D: %.2f Loss_G: %.2f Time: %.2fs'''\n                  % (epoch, self.max_epoch, self.num_batches,\n                     errD_total.item(), errG_total.item(),\n                     end_t - start_t))\n\n            if epoch % cfg.TRAIN.SNAPSHOT_INTERVAL == 0:  # and epoch != 0:\n                self.save_model(netG, avg_param_G, netsD, epoch)\n\n            D_total_loss = D_total_loss / step\n            G_total_loss = G_total_loss / step\n            # self.writer.add_scalar('Loss_D', D_total_loss , epoch  + 1)\n            # self.writer.add_scalar('Loss_G', G_total_loss , epoch  + 1)\n            self.writer.add_scalars('Loss_D and Loss_G', {'Loss_D': D_total_loss, 'Loss_G': G_total_loss}, epoch  + 1)\n\n        self.writer.close()\n\n        self.save_model(netG, avg_param_G, netsD, self.max_epoch)\n\n    def save_singleimages(self, images, filenames, save_dir,\n                          split_dir, sentenceID=0):\n        for i in range(images.size(0)):\n            s_tmp = '%s/single_samples/%s/%s' %\\\n                (save_dir, split_dir, filenames[i])\n            folder = s_tmp[:s_tmp.rfind('/')]\n            if not os.path.isdir(folder):\n                print('Make a new folder: ', folder)\n                mkdir_p(folder)\n\n            fullpath = '%s_%d.jpg' % (s_tmp, sentenceID)\n            # range from [-1, 1] to [0, 1]\n            # img = (images[i] + 1.0) / 2\n            img = images[i].add(1).div(2).mul(255).clamp(0, 255).byte()\n            # range from [0, 1] to [0, 255]\n            ndarr = img.permute(1, 2, 0).data.cpu().numpy()\n            im = Image.fromarray(ndarr)\n            im.save(fullpath)\n\n    def sampling(self, split_dir):\n        if cfg.DATASET_NAME=='birds':\n            from datasets import prepare_data\n        if cfg.DATASET_NAME=='flowers':\n            from datasets_flowers import prepare_data\n        if cfg.TRAIN.NET_G == '':\n            print('Error: the path for morels is not found!')\n        else:\n            if split_dir == 'test':\n                split_dir = 'valid'\n            # Build and load the generator\n            if cfg.GAN.B_DCGAN:\n                netG = G_DCGAN()\n            else:\n                netG = G_NET()\n            netG.apply(weights_init)\n            netG.cuda()\n            netG.eval()\n\n            # load text encoder\n            text_encoder = AlbertModel.from_pretrained(\"albert-base-v2\")\n            state_dict = torch.load(cfg.TRAIN.NET_E, map_location=lambda storage, loc: storage)\n            text_encoder.load_state_dict(state_dict)\n            print('Load text encoder from:', cfg.TRAIN.NET_E)\n            text_encoder = text_encoder.cuda()\n            text_encoder.eval()\n\n            #load image encoder\n            image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)\n            img_encoder_path = cfg.TRAIN.NET_E.replace('text_encoder', 'image_encoder')\n            state_dict = torch.load(img_encoder_path, map_location=lambda storage, loc: storage)\n            image_encoder.load_state_dict(state_dict)\n            print('Load image encoder from:', img_encoder_path)\n            image_encoder = image_encoder.cuda()\n            image_encoder.eval()\n\n            batch_size = self.batch_size\n            nz = cfg.GAN.Z_DIM\n            noise = Variable(torch.FloatTensor(batch_size, nz), volatile=True)\n            noise = noise.cuda()\n\n            model_dir = cfg.TRAIN.NET_G\n            state_dict = torch.load(model_dir, map_location=lambda storage, loc: storage)\n            # state_dict = torch.load(cfg.TRAIN.NET_G)\n            netG.load_state_dict(state_dict)\n            print('Load G from: ', model_dir)\n\n            # the path to save generated images\n            s_tmp = 'val_gen_images'\n            save_dir = '%s/%s' % (s_tmp, split_dir)\n            mkdir_p(save_dir)\n\n            cnt = 0\n            R_count = 0\n            R = np.zeros(30000)\n            cont = True\n            for ii in range(11):  # (cfg.TEXT.CAPTIONS_PER_IMAGE):\n                if (cont == False):\n                    break\n                for step, data in enumerate(self.data_loader, 0):\n                    cnt += batch_size\n                    if (cont == False):\n                        break\n                    if step % 100 == 0:\n                       print('cnt: ', cnt)\n                    # if step > 50:\n                    #     break\n\n                    # imgs, captions, cap_lens, class_ids, keys = prepare_data(data)\n\n                    imgs, imgs_2, captions, cap_lens, class_ids, keys, captions_2, cap_lens_2, class_ids_2, \\\n                    sort_ind, sort_ind_2 = prepare_data(data)\n\n                    words_embs = text_encoder( captions )[0].transpose(1, 2).contiguous()\n                    sent_emb = words_embs[ :, :, -1 ].contiguous()\n                    words_embs, sent_emb = words_embs.detach(), sent_emb.detach()\n                    mask = (captions == 0)\n                    num_words = words_embs.size(2)\n                    if mask.size(1) > num_words:\n                        mask = mask[:, :num_words]\n\n                    #######################################################\n                    # (2) Generate fake images\n                    ######################################################\n                    noise.data.normal_(0, 1)\n                    fake_imgs, _, _, _ = netG(noise, sent_emb, words_embs, mask)\n                    for j in range(batch_size):\n                        s_tmp = '%s/single/%s' % (save_dir, keys[j])\n                        folder = s_tmp[:s_tmp.rfind('/')]\n                        if not os.path.isdir(folder):\n                            #print('Make a new folder: ', folder)\n                            mkdir_p(folder)\n                        k = -1\n                        # for k in range(len(fake_imgs)):\n                        im = fake_imgs[k][j].data.cpu().numpy()\n                        # [-1, 1] --> [0, 255]\n                        im = (im + 1.0) * 127.5\n                        im = im.astype(np.uint8)\n                        im = np.transpose(im, (1, 2, 0))\n                        im = Image.fromarray(im)\n                        fullpath = '%s_s%d_%d.png' % (s_tmp, k, ii)\n                        im.save(fullpath)\n\n                    _, cnn_code = image_encoder(fake_imgs[-1])\n\n                    for i in range(batch_size):\n                        mis_captions, mis_captions_len = self.dataset.get_mis_caption(class_ids[i])\n                        words_embs_t = text_encoder(mis_captions )[0].transpose(1, 2).contiguous()\n                        sent_emb_t = words_embs[ :, :, -1 ].contiguous()\n                        rnn_code = torch.cat((sent_emb[i, :].unsqueeze(0), sent_emb_t), 0)\n                        ### cnn_code = 1 * nef\n                        ### rnn_code = 100 * nef\n                        scores = torch.mm(cnn_code[i].unsqueeze(0), rnn_code.transpose(0, 1))  # 1* 100\n                        cnn_code_norm = torch.norm(cnn_code[i].unsqueeze(0), 2, dim=1, keepdim=True)\n                        rnn_code_norm = torch.norm(rnn_code, 2, dim=1, keepdim=True)\n                        norm = torch.mm(cnn_code_norm, rnn_code_norm.transpose(0, 1))\n                        scores0 = scores / norm.clamp(min=1e-8)\n                        if torch.argmax(scores0) == 0:\n                            R[R_count] = 1\n                        R_count += 1\n\n                    if R_count >= 30000:\n                        sum = np.zeros(10)\n                        np.random.shuffle(R)\n                        for i in range(10):\n                            sum[i] = np.average(R[i * 3000:(i + 1) * 3000 - 1])\n                        R_mean = np.average(sum)\n                        R_std = np.std(sum)\n                        print(\"R mean:{:.4f} std:{:.4f}\".format(R_mean, R_std))\n                        cont = False\n\n    def gen_example(self, data_dic):\n        if cfg.TRAIN.NET_G == '':\n            print('Error: the path for morels is not found!')\n        else:\n            # Build and load the generator\n            text_encoder = AlbertModel.from_pretrained(\"albert-base-v2\")\n            state_dict = \\\n                torch.load(cfg.TRAIN.NET_E, map_location=lambda storage, loc: storage)\n            text_encoder.load_state_dict(state_dict)\n            print('Load text encoder from:', cfg.TRAIN.NET_E)\n            text_encoder = text_encoder.cuda()\n            text_encoder.eval()\n\n            # the path to save generated images\n            if cfg.GAN.B_DCGAN:\n                netG = G_DCGAN()\n            else:\n                netG = G_NET()\n            s_tmp = 'example_images'\n            model_dir = cfg.TRAIN.NET_G\n            state_dict = \\\n                torch.load(model_dir, map_location=lambda storage, loc: storage)\n            netG.load_state_dict(state_dict)\n            print('Load G from: ', model_dir)\n            netG.cuda()\n            netG.eval()\n            for key in data_dic:\n                save_dir = '%s/%s' % (s_tmp, key)\n                mkdir_p(save_dir)\n                captions, cap_lens, sorted_indices = data_dic[key]\n\n                batch_size = captions.shape[0]\n                nz = cfg.GAN.Z_DIM\n                captions = Variable(torch.from_numpy(captions), volatile=True)\n                cap_lens = Variable(torch.from_numpy(cap_lens), volatile=True)\n\n                captions = captions.cuda()\n                cap_lens = cap_lens.cuda()\n                for i in range(1):  # 16\n                    noise = Variable(torch.FloatTensor(batch_size, nz), volatile=True)\n                    noise = noise.cuda()\n                    #######################################################\n                    # (1) Extract text embeddings\n                    ######################################################\n                    words_embs = text_encoder( captions )[0].transpose(1, 2).contiguous()\n                    sent_emb = words_embs[ :, :, -1 ].contiguous()\n                    mask = (captions == 0)\n                    #######################################################\n                    # (2) Generate fake images\n                    ######################################################\n                    noise.data.normal_(0, 1)\n                    fake_imgs, attention_maps, _, _ = netG(noise, sent_emb, words_embs, mask)\n                    # G attention\n                    cap_lens_np = cap_lens.cpu().data.numpy()\n                    for j in range(batch_size):\n                        save_name = '%s/%d_s_%d' % (save_dir, i, sorted_indices[j])\n                        for k in range(len(fake_imgs)):\n                            im = fake_imgs[k][j].data.cpu().numpy()\n                            im = (im + 1.0) * 127.5\n                            im = im.astype(np.uint8)\n                            # print('im', im.shape)\n                            im = np.transpose(im, (1, 2, 0))\n                            # print('im', im.shape)\n                            im = Image.fromarray(im)\n                            fullpath = '%s_g%d.png' % (save_name, k)\n                            im.save(fullpath)\n\n                        for k in range(len(attention_maps)):\n                            if len(fake_imgs) > 1:\n                                im = fake_imgs[k + 1].detach().cpu()\n                            else:\n                                im = fake_imgs[0].detach().cpu()\n                            attn_maps = attention_maps[k]\n                            att_sze = attn_maps.size(2)\n                            img_set, sentences = \\\n                                build_super_images2(im[j].unsqueeze(0),\n                                                    captions[j].unsqueeze(0),\n                                                    [cap_lens_np[j]], self.ixtoword,\n                                                    [attn_maps[j]], att_sze)\n                            if img_set is not None:\n                                im = Image.fromarray(img_set)\n                                fullpath = '%s_a%d.png' % (save_name, k)\n                        ","metadata":{"execution":{"iopub.status.busy":"2023-05-11T09:15:31.578146Z","iopub.execute_input":"2023-05-11T09:15:31.578531Z","iopub.status.idle":"2023-05-11T09:15:31.606849Z","shell.execute_reply.started":"2023-05-11T09:15:31.578499Z","shell.execute_reply":"2023-05-11T09:15:31.605572Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Overwriting code/trainer.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Download the checkpoints of DAMSM phase and GAN training phase from here: https://drive.google.com/drive/folders/1ok6XFN3gZATmbhBSrIVXdiOv3evqMt5O?usp=share_link","metadata":{}},{"cell_type":"markdown","source":"**Experiment 1 (RoBERTa transformer + CL2(GAN training))**","metadata":{}},{"cell_type":"markdown","source":"1. Clone the repository\n2. Make the following changes in model.py, pretrain_damsm.py and trainer.py files.\n3. Follow the same procedure for DAMSM training, GAN training and evaluation.","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/mansivv9/Text_to_image_synthesis_Major_Project.git","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.chdir('/kaggle/working/Text_to_image_synthesis_Major_Project/AttnGAN+CL+SN+RoBERTa(AttnGAN_V2)')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile code/model.py\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nfrom torch.autograd import Variable\nfrom torchvision import models\nimport torch.utils.model_zoo as model_zoo\nimport torch.nn.functional as F\n\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nfrom miscc.config import cfg\nfrom GlobalAttention import GlobalAttentionGeneral as ATT_NET\n\n# import spacy\n# spacy.prefer_gpu()\n# torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n############################## Bert ##################################################\n\n# class BERT_encoder:    \n#     def __init__(self):\n#         # self._init_processors()\n#         self.bert_model = spacy.load(\"en_trf_distilbertbaseuncased_lg\")\n#     def __call__(self, texts):\n#         docs = self.bert_model(texts)\n#         sentence_emb = docs.vector\n#         word_vectors = [w.vector for w in docs]\n#         return sentence_emb, word_vectors\n            \n\n############################## Image Caption ########################################\n#####################################################################################\nclass GLU(nn.Module):\n    def __init__(self):\n        super(GLU, self).__init__()\n\n    def forward(self, x):\n        nc = x.size(1)\n        assert nc % 2 == 0, 'channels dont divide 2!'\n        nc = int(nc/2)\n        return x[:, :nc] * torch.sigmoid(x[:, nc:])\n\nclass Interpolate(nn.Module):\n    def __init__(self, scale_factor, mode, size=None):\n        super(Interpolate, self).__init__()\n        self.interp = nn.functional.interpolate\n        self.scale_factor = scale_factor\n        self.mode = mode\n        self.size = size\n\n    def forward(self, x):\n        x = self.interp(x, scale_factor=self.scale_factor, mode=self.mode, size=self.size)\n        return x\n\n\ndef conv1x1(in_planes, out_planes, bias=False):\n    \"1x1 convolution with padding\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,\n                     padding=0, bias=bias)\n\n\ndef conv3x3(in_planes, out_planes):\n    \"3x3 convolution with padding\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=1,\n                     padding=1, bias=False)\n\n\n# Upsale the spatial size by a factor of 2\ndef upBlock(in_planes, out_planes):\n    block = nn.Sequential(\n        Interpolate(scale_factor=2, mode='nearest'),\n        conv3x3(in_planes, out_planes * 2),\n        nn.BatchNorm2d(out_planes * 2),\n        GLU())\n    return block\n\n\n# Keep the spatial size\ndef Block3x3_relu(in_planes, out_planes):\n    block = nn.Sequential(\n        conv3x3(in_planes, out_planes * 2),\n        nn.BatchNorm2d(out_planes * 2),\n        GLU())\n    return block\n\n\nclass ResBlock(nn.Module):\n    def __init__(self, channel_num):\n        super(ResBlock, self).__init__()\n        self.block = nn.Sequential(\n            conv3x3(channel_num, channel_num * 2),\n            nn.BatchNorm2d(channel_num * 2),\n            GLU(),\n            conv3x3(channel_num, channel_num),\n            nn.BatchNorm2d(channel_num))\n\n    def forward(self, x):\n        residual = x\n        out = self.block(x)\n        out += residual\n        return out\n\n\n# ############## Text2Image Encoder-Decoder #######\nTRANSFORMER_ENCODER = 'roberta-base'\nclass RNN_ENCODER(nn.Module):\n    def __init__(self, ntoken, ninput=300, drop_prob=0.5,\n                 nhidden=128, nlayers=1, bidirectional=True):\n        super(RNN_ENCODER, self).__init__()\n        self.n_steps = cfg.TEXT.WORDS_NUM\n        self.ntoken = ntoken  # size of the dictionary\n        self.ninput = ninput  # size of each embedding vector\n        self.drop_prob = drop_prob  # probability of an element to be zeroed\n        self.nlayers = nlayers  # Number of recurrent layers\n        self.bidirectional = bidirectional\n        self.rnn_type = cfg.RNN_TYPE\n        if bidirectional:\n            self.num_directions = 2\n        else:\n            self.num_directions = 1\n        # number of features in the hidden state\n        self.nhidden = nhidden // self.num_directions\n\n        self.define_module()\n        self.init_weights()\n\n    def define_module(self):\n        self.encoder = nn.Embedding(self.ntoken, self.ninput)\n        self.drop = nn.Dropout(self.drop_prob)\n        if self.rnn_type == 'LSTM':\n            # dropout: If non-zero, introduces a dropout layer on\n            # the outputs of each RNN layer except the last layer\n            self.rnn = nn.LSTM(self.ninput, self.nhidden,\n                               self.nlayers, batch_first=True,\n                               dropout=self.drop_prob,\n                               bidirectional=self.bidirectional)\n        elif self.rnn_type == 'GRU':\n            self.rnn = nn.GRU(self.ninput, self.nhidden,\n                              self.nlayers, batch_first=True,\n                              dropout=self.drop_prob,\n                              bidirectional=self.bidirectional)\n        else:\n            raise NotImplementedError\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        # Do not need to initialize RNN parameters, which have been initialized\n        # http://pytorch.org/docs/master/_modules/torch/nn/modules/rnn.html#LSTM\n        # self.decoder.weight.data.uniform_(-initrange, initrange)\n        # self.decoder.bias.data.fill_(0)\n\n    def init_hidden(self, bsz):\n        weight = next(self.parameters()).data\n        if self.rnn_type == 'LSTM':\n            return (Variable(weight.new(self.nlayers * self.num_directions,\n                                        bsz, self.nhidden).zero_()),\n                    Variable(weight.new(self.nlayers * self.num_directions,\n                                        bsz, self.nhidden).zero_()))\n        else:\n            return Variable(weight.new(self.nlayers * self.num_directions,\n                                       bsz, self.nhidden).zero_())\n\n    def forward(self, captions, cap_lens, hidden, mask=None):\n        # input: torch.LongTensor of size batch x n_steps\n        # --> emb: batch x n_steps x ninput\n        emb = self.drop(self.encoder(captions))\n        #\n        # Returns: a PackedSequence object\n        cap_lens = cap_lens.data.tolist()\n        emb = pack_padded_sequence(emb, cap_lens, batch_first=True)\n        # #hidden and memory (num_layers * num_directions, batch, hidden_size):\n        # tensor containing the initial hidden state for each element in batch.\n        # #output (batch, seq_len, hidden_size * num_directions)\n        # #or a PackedSequence object:\n        # tensor containing output features (h_t) from the last layer of RNN\n        output, hidden = self.rnn(emb, hidden)\n        # PackedSequence object\n        # --> (batch, seq_len, hidden_size * num_directions)\n        output = pad_packed_sequence(output, batch_first=True)[0]\n        # output = self.drop(output)\n        # --> batch x hidden_size*num_directions x seq_len\n        words_emb = output.transpose(1, 2)\n        # --> batch x num_directions*hidden_size\n        if self.rnn_type == 'LSTM':\n            sent_emb = hidden[0].transpose(0, 1).contiguous()\n        else:\n            sent_emb = hidden.transpose(0, 1).contiguous()\n        sent_emb = sent_emb.view(-1, self.nhidden * self.num_directions)\n        return words_emb, sent_emb\n\n\nclass CNN_ENCODER(nn.Module):\n    def __init__(self, nef):\n        super(CNN_ENCODER, self).__init__()\n        if cfg.TRAIN.FLAG:\n            self.nef = nef\n        else:\n            self.nef = 768  # define a uniform ranker\n\n        model = models.inception_v3()\n        url = 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth'\n        #densenet201': 'https://download.pytorch.org/models/densenet201-c1103571.pth'\n        #models.densenet201()\n        model.load_state_dict(model_zoo.load_url(url))\n        for param in model.parameters():\n            param.requires_grad = False\n        print('Load pretrained model from ', url)\n        # print(model)\n\n        self.define_module(model)\n        self.init_trainable_weights()\n\n    def define_module(self, model):\n        self.Conv2d_1a_3x3 = model.Conv2d_1a_3x3\n        self.Conv2d_2a_3x3 = model.Conv2d_2a_3x3\n        self.Conv2d_2b_3x3 = model.Conv2d_2b_3x3\n        self.Conv2d_3b_1x1 = model.Conv2d_3b_1x1\n        self.Conv2d_4a_3x3 = model.Conv2d_4a_3x3\n        self.Mixed_5b = model.Mixed_5b\n        self.Mixed_5c = model.Mixed_5c\n        self.Mixed_5d = model.Mixed_5d\n        self.Mixed_6a = model.Mixed_6a\n        self.Mixed_6b = model.Mixed_6b\n        self.Mixed_6c = model.Mixed_6c\n        self.Mixed_6d = model.Mixed_6d\n        self.Mixed_6e = model.Mixed_6e\n        self.Mixed_7a = model.Mixed_7a\n        self.Mixed_7b = model.Mixed_7b\n        self.Mixed_7c = model.Mixed_7c\n\n        self.emb_features = conv1x1(768, self.nef)\n        self.emb_cnn_code = nn.Linear(2048, self.nef)\n\n    def init_trainable_weights(self):\n        initrange = 0.1\n        self.emb_features.weight.data.uniform_(-initrange, initrange)\n        self.emb_cnn_code.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, x):\n        features = None\n        # --> fixed-size input: batch x 3 x 299 x 299\n        x = nn.functional.interpolate(x,size=(299, 299), mode='bilinear', align_corners=False)\n        # 299 x 299 x 3\n        x = self.Conv2d_1a_3x3(x)\n        # 149 x 149 x 32\n        x = self.Conv2d_2a_3x3(x)\n        # 147 x 147 x 32\n        x = self.Conv2d_2b_3x3(x)\n        # 147 x 147 x 64\n        x = F.max_pool2d(x, kernel_size=3, stride=2)\n        # 73 x 73 x 64\n        x = self.Conv2d_3b_1x1(x)\n        # 73 x 73 x 80\n        x = self.Conv2d_4a_3x3(x)\n        # 71 x 71 x 192\n\n        x = F.max_pool2d(x, kernel_size=3, stride=2)\n        # 35 x 35 x 192\n        x = self.Mixed_5b(x)\n        # 35 x 35 x 256\n        x = self.Mixed_5c(x)\n        # 35 x 35 x 288\n        x = self.Mixed_5d(x)\n        # 35 x 35 x 288\n\n        x = self.Mixed_6a(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6b(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6c(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6d(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6e(x)\n        # 17 x 17 x 768\n\n        # image region features\n        features = x\n        # 17 x 17 x 768\n\n        x = self.Mixed_7a(x)\n        # 8 x 8 x 1280\n        x = self.Mixed_7b(x)\n        # 8 x 8 x 2048\n        x = self.Mixed_7c(x)\n        # 8 x 8 x 2048\n        x = F.avg_pool2d(x, kernel_size=8)\n        # 1 x 1 x 2048\n        # x = F.dropout(x, training=self.training)\n        # 1 x 1 x 2048\n        x = x.view(x.size(0), -1)\n        # 2048\n\n        # global image features\n        cnn_code = self.emb_cnn_code(x)\n        # 512\n        if features is not None:\n            features = self.emb_features(features)\n        return features, cnn_code\n\n\n# ############## G networks ###################\nclass CA_NET(nn.Module):\n    # some code is modified from vae examples\n    # (https://github.com/pytorch/examples/blob/master/vae/main.py)\n    def __init__(self):\n        super(CA_NET, self).__init__()\n        self.t_dim = cfg.TEXT.EMBEDDING_DIM\n        self.c_dim = cfg.GAN.CONDITION_DIM\n        self.fc = nn.Linear(self.t_dim, self.c_dim * 4, bias=True)\n        self.relu = GLU()\n\n    def encode(self, text_embedding):\n        x = self.relu(self.fc(text_embedding))\n        mu = x[:, :self.c_dim]\n        logvar = x[:, self.c_dim:]\n        return mu, logvar\n\n    def reparametrize(self, mu, logvar):\n        std = logvar.mul(0.5).exp_()\n        if cfg.CUDA:\n            eps = torch.cuda.FloatTensor(std.size()).normal_()\n        else:\n            eps = torch.FloatTensor(std.size()).normal_()\n        eps = Variable(eps)\n        return eps.mul(std).add_(mu)\n\n    def forward(self, text_embedding):\n        mu, logvar = self.encode(text_embedding)\n        c_code = self.reparametrize(mu, logvar)\n        return c_code, mu, logvar\n\n\nclass INIT_STAGE_G(nn.Module):\n    def __init__(self, ngf, ncf):\n        super(INIT_STAGE_G, self).__init__()\n        self.gf_dim = ngf\n        self.in_dim = cfg.GAN.Z_DIM + ncf  # cfg.TEXT.EMBEDDING_DIM\n\n        self.define_module()\n\n    def define_module(self):\n        nz, ngf = self.in_dim, self.gf_dim\n        self.fc = nn.Sequential(\n            nn.Linear(nz, ngf * 4 * 4 * 2, bias=False),\n            nn.BatchNorm1d(ngf * 4 * 4 * 2),\n            GLU())\n\n        self.upsample1 = upBlock(ngf, ngf // 2)\n        self.upsample2 = upBlock(ngf // 2, ngf // 4)\n        self.upsample3 = upBlock(ngf // 4, ngf // 8)\n        self.upsample4 = upBlock(ngf // 8, ngf // 16)\n\n    def forward(self, z_code, c_code):\n        \"\"\"\n        :param z_code: batch x cfg.GAN.Z_DIM\n        :param c_code: batch x cfg.TEXT.EMBEDDING_DIM\n        :return: batch x ngf/16 x 64 x 64\n        \"\"\"\n        c_z_code = torch.cat((c_code, z_code), 1)\n        # state size ngf x 4 x 4\n        out_code = self.fc(c_z_code)\n        out_code = out_code.view(-1, self.gf_dim, 4, 4)\n        # state size ngf/3 x 8 x 8\n        out_code = self.upsample1(out_code)\n        # state size ngf/4 x 16 x 16\n        out_code = self.upsample2(out_code)\n        # state size ngf/8 x 32 x 32\n        out_code32 = self.upsample3(out_code)\n        # state size ngf/16 x 64 x 64\n        out_code64 = self.upsample4(out_code32)\n\n        return out_code64\n\n\nclass NEXT_STAGE_G(nn.Module):\n    def __init__(self, ngf, nef, ncf):\n        super(NEXT_STAGE_G, self).__init__()\n        self.gf_dim = ngf\n        self.ef_dim = nef\n        self.cf_dim = ncf\n        self.num_residual = cfg.GAN.R_NUM\n        self.define_module()\n\n    def _make_layer(self, block, channel_num):\n        layers = []\n        for i in range(cfg.GAN.R_NUM):\n            layers.append(block(channel_num))\n        return nn.Sequential(*layers)\n\n    def define_module(self):\n        ngf = self.gf_dim\n        self.att = ATT_NET(ngf, self.ef_dim)\n        self.residual = self._make_layer(ResBlock, ngf * 2)\n        self.upsample = upBlock(ngf * 2, ngf)\n\n    def forward(self, h_code, c_code, word_embs, mask):\n        \"\"\"\n            h_code1(query):  batch x idf x ih x iw (queryL=ihxiw)\n            word_embs(context): batch x cdf x sourceL (sourceL=seq_len)\n            c_code1: batch x idf x queryL\n            att1: batch x sourceL x queryL\n        \"\"\"\n        self.att.applyMask(mask)\n        c_code, att = self.att(h_code, word_embs)\n        h_c_code = torch.cat((h_code, c_code), 1)\n        out_code = self.residual(h_c_code)\n\n        # state size ngf/2 x 2in_size x 2in_size\n        out_code = self.upsample(out_code)\n\n        return out_code, att\n\n\nclass GET_IMAGE_G(nn.Module):\n    def __init__(self, ngf):\n        super(GET_IMAGE_G, self).__init__()\n        self.gf_dim = ngf\n        self.img = nn.Sequential(\n            conv3x3(ngf, 3),\n            nn.Tanh()\n        )\n\n    def forward(self, h_code):\n        out_img = self.img(h_code)\n        return out_img\n\n\nclass G_NET(nn.Module):\n    def __init__(self):\n        super(G_NET, self).__init__()\n        ngf = cfg.GAN.GF_DIM\n        nef = cfg.TEXT.EMBEDDING_DIM\n        ncf = cfg.GAN.CONDITION_DIM\n        self.ca_net = CA_NET()\n\n        if cfg.TREE.BRANCH_NUM > 0:\n            self.h_net1 = INIT_STAGE_G(ngf * 16, ncf)\n            self.img_net1 = GET_IMAGE_G(ngf)\n        # gf x 64 x 64\n        if cfg.TREE.BRANCH_NUM > 1:\n            self.h_net2 = NEXT_STAGE_G(ngf, nef, ncf)\n            self.img_net2 = GET_IMAGE_G(ngf)\n        if cfg.TREE.BRANCH_NUM > 2:\n            self.h_net3 = NEXT_STAGE_G(ngf, nef, ncf)\n            self.img_net3 = GET_IMAGE_G(ngf)\n\n    def forward(self, z_code, sent_emb, word_embs, mask):\n        \"\"\"\n            :param z_code: batch x cfg.GAN.Z_DIM\n            :param sent_emb: batch x cfg.TEXT.EMBEDDING_DIM\n            :param word_embs: batch x cdf x seq_len\n            :param mask: batch x seq_len\n            :return:\n        \"\"\"\n        fake_imgs = []\n        att_maps = []\n        c_code, mu, logvar = self.ca_net(sent_emb)\n\n        if cfg.TREE.BRANCH_NUM > 0:\n            h_code1 = self.h_net1(z_code, c_code)\n            fake_img1 = self.img_net1(h_code1)\n            fake_imgs.append(fake_img1)\n        if cfg.TREE.BRANCH_NUM > 1:\n            h_code2, att1 = \\\n                self.h_net2(h_code1, c_code, word_embs, mask)\n            fake_img2 = self.img_net2(h_code2)\n            fake_imgs.append(fake_img2)\n            if att1 is not None:\n                att_maps.append(att1)\n        if cfg.TREE.BRANCH_NUM > 2:\n            h_code3, att2 = \\\n                self.h_net3(h_code2, c_code, word_embs, mask)\n            fake_img3 = self.img_net3(h_code3)\n            fake_imgs.append(fake_img3)\n            if att2 is not None:\n                att_maps.append(att2)\n\n        return fake_imgs, att_maps, mu, logvar\n\n\n\nclass G_DCGAN(nn.Module):\n    def __init__(self):\n        super(G_DCGAN, self).__init__()\n        ngf = cfg.GAN.GF_DIM\n        nef = cfg.TEXT.EMBEDDING_DIM\n        ncf = cfg.GAN.CONDITION_DIM\n        self.ca_net = CA_NET()\n\n        # 16gf x 64 x 64 --> gf x 64 x 64 --> 3 x 64 x 64\n        if cfg.TREE.BRANCH_NUM > 0:\n            self.h_net1 = INIT_STAGE_G(ngf * 16, ncf)\n        # gf x 64 x 64\n        if cfg.TREE.BRANCH_NUM > 1:\n            self.h_net2 = NEXT_STAGE_G(ngf, nef, ncf)\n        if cfg.TREE.BRANCH_NUM > 2:\n            self.h_net3 = NEXT_STAGE_G(ngf, nef, ncf)\n        self.img_net = GET_IMAGE_G(ngf)\n\n    def forward(self, z_code, sent_emb, word_embs, mask):\n        \"\"\"\n            :param z_code: batch x cfg.GAN.Z_DIM\n            :param sent_emb: batch x cfg.TEXT.EMBEDDING_DIM\n            :param word_embs: batch x cdf x seq_len\n            :param mask: batch x seq_len\n            :return:\n        \"\"\"\n        att_maps = []\n        c_code, mu, logvar = self.ca_net(sent_emb)\n        if cfg.TREE.BRANCH_NUM > 0:\n            h_code = self.h_net1(z_code, c_code)\n        if cfg.TREE.BRANCH_NUM > 1:\n            h_code, att1 = self.h_net2(h_code, c_code, word_embs, mask)\n            if att1 is not None:\n                att_maps.append(att1)\n        if cfg.TREE.BRANCH_NUM > 2:\n            h_code, att2 = self.h_net3(h_code, c_code, word_embs, mask)\n            if att2 is not None:\n                att_maps.append(att2)\n\n        fake_imgs = self.img_net(h_code)\n        return [fake_imgs], att_maps, mu, logvar\n\n\n# ############## D networks ##########################\ndef Block3x3_leakRelu(in_planes, out_planes):\n    block = nn.Sequential(\n        conv3x3(in_planes, out_planes),\n        nn.BatchNorm2d(out_planes),\n        nn.LeakyReLU(0.2, inplace=True)\n    )\n    return block\n\n\n# Downsale the spatial size by a factor of 2\ndef downBlock(in_planes, out_planes):\n    block = nn.Sequential(\n        nn.Conv2d(in_planes, out_planes, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(out_planes),\n        nn.LeakyReLU(0.2, inplace=True)\n    )\n    return block\n\n\n# Downsale the spatial size by a factor of 16\ndef encode_image_by_16times(ndf):\n    encode_img = nn.Sequential(\n        # --> state size. ndf x in_size/2 x in_size/2\n        nn.Conv2d(3, ndf, 4, 2, 1, bias=False),\n        nn.LeakyReLU(0.2, inplace=True),\n        # --> state size 2ndf x x in_size/4 x in_size/4\n        nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ndf * 2),\n        nn.LeakyReLU(0.2, inplace=True),\n        # --> state size 4ndf x in_size/8 x in_size/8\n        nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ndf * 4),\n        nn.LeakyReLU(0.2, inplace=True),\n        # --> state size 8ndf x in_size/16 x in_size/16\n        nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ndf * 8),\n        nn.LeakyReLU(0.2, inplace=True)\n    )\n    return encode_img\n\n\nclass D_GET_LOGITS(nn.Module):\n    def __init__(self, ndf, nef, bcondition=False):\n        super(D_GET_LOGITS, self).__init__()\n        self.df_dim = ndf\n        self.ef_dim = nef\n        self.bcondition = bcondition\n        if self.bcondition:\n            self.jointConv = Block3x3_leakRelu(ndf * 8 + nef, ndf * 8)\n\n        self.outlogits = nn.Sequential(\n            nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=4),\n            nn.Sigmoid())\n\n    def forward(self, h_code, c_code=None):\n        if self.bcondition and c_code is not None:\n            # conditioning output\n            c_code = c_code.view(-1, self.ef_dim, 1, 1)\n            c_code = c_code.repeat(1, 1, 4, 4)\n            # state size (ngf+egf) x 4 x 4\n            h_c_code = torch.cat((h_code, c_code), 1)\n            # state size ngf x in_size x in_size\n            h_c_code = self.jointConv(h_c_code)\n        else:\n            h_c_code = h_code\n\n        output = self.outlogits(h_c_code)\n        return output.view(-1)\n\n\n# For 64 x 64 images\nclass D_NET64(nn.Module):\n    def __init__(self, b_jcu=True):\n        super(D_NET64, self).__init__()\n        ndf = cfg.GAN.DF_DIM\n        nef = cfg.TEXT.EMBEDDING_DIM\n        self.img_code_s16 = encode_image_by_16times(ndf)\n        if b_jcu:\n            self.UNCOND_DNET = D_GET_LOGITS(ndf, nef, bcondition=False)\n        else:\n            self.UNCOND_DNET = None\n        self.COND_DNET = D_GET_LOGITS(ndf, nef, bcondition=True)\n\n    def forward(self, x_var):\n        x_code4 = self.img_code_s16(x_var)  # 4 x 4 x 8df\n        return x_code4\n\n\n# For 128 x 128 images\nclass D_NET128(nn.Module):\n    def __init__(self, b_jcu=True):\n        super(D_NET128, self).__init__()\n        ndf = cfg.GAN.DF_DIM\n        nef = cfg.TEXT.EMBEDDING_DIM\n        self.img_code_s16 = encode_image_by_16times(ndf)\n        self.img_code_s32 = downBlock(ndf * 8, ndf * 16)\n        self.img_code_s32_1 = Block3x3_leakRelu(ndf * 16, ndf * 8)\n        #\n        if b_jcu:\n            self.UNCOND_DNET = D_GET_LOGITS(ndf, nef, bcondition=False)\n        else:\n            self.UNCOND_DNET = None\n        self.COND_DNET = D_GET_LOGITS(ndf, nef, bcondition=True)\n\n    def forward(self, x_var):\n        x_code8 = self.img_code_s16(x_var)   # 8 x 8 x 8df\n        x_code4 = self.img_code_s32(x_code8)   # 4 x 4 x 16df\n        x_code4 = self.img_code_s32_1(x_code4)  # 4 x 4 x 8df\n        return x_code4\n\n\n# For 256 x 256 images\nclass D_NET256(nn.Module):\n    def __init__(self, b_jcu=True):\n        super(D_NET256, self).__init__()\n        ndf = cfg.GAN.DF_DIM\n        nef = cfg.TEXT.EMBEDDING_DIM\n        self.img_code_s16 = encode_image_by_16times(ndf)\n        self.img_code_s32 = downBlock(ndf * 8, ndf * 16)\n        self.img_code_s64 = downBlock(ndf * 16, ndf * 32)\n        self.img_code_s64_1 = Block3x3_leakRelu(ndf * 32, ndf * 16)\n        self.img_code_s64_2 = Block3x3_leakRelu(ndf * 16, ndf * 8)\n        if b_jcu:\n            self.UNCOND_DNET = D_GET_LOGITS(ndf, nef, bcondition=False)\n        else:\n            self.UNCOND_DNET = None\n        self.COND_DNET = D_GET_LOGITS(ndf, nef, bcondition=True)\n\n    def forward(self, x_var):\n        x_code16 = self.img_code_s16(x_var)\n        x_code8 = self.img_code_s32(x_code16)\n        x_code4 = self.img_code_s64(x_code8)\n        x_code4 = self.img_code_s64_1(x_code4)\n        x_code4 = self.img_code_s64_2(x_code4)\n        return x_code4","metadata":{"execution":{"iopub.status.busy":"2023-05-11T10:07:32.761026Z","iopub.execute_input":"2023-05-11T10:07:32.761432Z","iopub.status.idle":"2023-05-11T10:07:32.786891Z","shell.execute_reply.started":"2023-05-11T10:07:32.761397Z","shell.execute_reply":"2023-05-11T10:07:32.785673Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Overwriting code/model.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile code/pretrain_DAMSM.py\nfrom __future__ import print_function\n\nfrom miscc.utils import mkdir_p\nfrom miscc.utils import build_super_images\nfrom miscc.losses import sent_loss, words_loss\nfrom miscc.config import cfg, cfg_from_file\n\nfrom model import TRANSFORMER_ENCODER, RNN_ENCODER, CNN_ENCODER\n\nimport os\nimport sys\nimport time\nimport random\nimport pprint\nimport datetime\nimport dateutil.tz\nimport argparse\nimport numpy as np\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as transforms\n# from torch.nn.utils.rnn import pad_packed_sequence\n\nfrom transformers import RobertaModel\n\n\ndir_path = (os.path.abspath(os.path.join(os.path.realpath(__file__), './.')))\nsys.path.append(dir_path)\n\n\nUPDATE_INTERVAL = 50\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Train a DAMSM network')\n    parser.add_argument('--cfg', dest='cfg_file',\n                        help='optional config file',\n                        default='cfg/DAMSM/bird.yml', type=str)\n    parser.add_argument('--gpu', dest='gpu_id', type=int, default=0)\n    parser.add_argument('--data_dir', dest='data_dir', type=str, default='')\n    parser.add_argument('--manualSeed', type=int, help='manual seed')\n    args = parser.parse_args()\n    return args\n\n\ndef train( dataloader, cnn_model, nlp_model, batch_size,\n           labels, optimizer, epoch, ixtoword, image_dir ):\n    \n    if cfg.DATASET_NAME=='birds':\n        from datasets import TextDataset\n        from datasets import prepare_data\n    if cfg.DATASET_NAME=='flowers':\n        from datasets_flowers import TextDataset\n        from datasets_flowers import prepare_data\n        \n    cnn_model.train()\n    nlp_model.train()\n    s_total_loss0 = 0\n    s_total_loss1 = 0\n    w_total_loss0 = 0\n    w_total_loss1 = 0\n    count = (epoch + 1) * len(dataloader)\n    start_time = time.time()\n    for step, data in enumerate(dataloader, 0):\n        # print('step', step)\n        nlp_model.zero_grad()\n        cnn_model.zero_grad()\n\n        imgs, imgs_2, captions, cap_lens, class_ids, keys, captions_2, cap_lens_2, class_ids_2, \\\n        sort_ind, sort_ind_2 = prepare_data(data)\n\n\n        # words_features: batch_size x nef x 17 x 17\n        # sent_code: batch_size x nef\n        words_features, sent_code = cnn_model(imgs[-1])\n        # print( words_features.shape, sent_code.shape )\n        # --> batch_size x nef x 17*17\n        nef, att_sze = words_features.size(1), words_features.size(2)\n        # words_features = words_features.view(batch_size, nef, -1)\n\n        # Forward Prop:\n        # inputs:\n        #   captions: torch.LongTensor of ids of size batch x n_steps\n        # outputs:\n        #   words_emb: batch_size x nef x seq_len\n        #   sent_emb: batch_size x nef\n        \n        words_emb = nlp_model( captions )[0].transpose(1, 2).contiguous()\n        sent_emb = words_emb[ :, :, -1 ].contiguous()\n            # sent_emb = sent_emb.view(batch_size, -1)\n        # print( words_emb.shape, sent_emb.shape )\n\n        # Compute Loss:\n        # NOTE: the ideal loss for Transformer may be different than that for bi-directional LSTM\n        w_loss0, w_loss1, attn_maps = words_loss( words_features, words_emb, labels,\n                                                  cap_lens, class_ids, batch_size )\n        w_total_loss0 += w_loss0.data\n        w_total_loss1 += w_loss1.data\n        loss = w_loss0 + w_loss1\n\n        s_loss0, s_loss1 = \\\n            sent_loss( sent_code, sent_emb, labels, class_ids, batch_size )\n        loss += s_loss0 + s_loss1\n        s_total_loss0 += s_loss0.data\n        s_total_loss1 += s_loss1.data\n        #\n        # Backprop:\n        loss.backward()\n        #\n        # `clip_grad_norm` helps prevent\n        # the exploding gradient problem in RNNs / LSTMs.\n        optimizer.step()\n\n        if step % UPDATE_INTERVAL == 0:\n            count = epoch * len(dataloader) + step\n\n            # print(  s_total_loss0, s_total_loss1 )\n            s_cur_loss0 = s_total_loss0.item() / UPDATE_INTERVAL\n            s_cur_loss1 = s_total_loss1.item() / UPDATE_INTERVAL\n\n            # print(  w_total_loss0, w_total_loss1 )\n            w_cur_loss0 = w_total_loss0.item() / UPDATE_INTERVAL\n            w_cur_loss1 = w_total_loss1.item() / UPDATE_INTERVAL\n\n            elapsed = time.time() - start_time\n            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n                  's_loss {:5.5f} {:5.5f} | '\n                  'w_loss {:5.5f} {:5.5f}'\n                  .format(epoch, step, len(dataloader),\n                          elapsed * 1000. / UPDATE_INTERVAL,\n                          s_cur_loss0, s_cur_loss1,\n                          w_cur_loss0, w_cur_loss1))\n            s_total_loss0 = 0\n            s_total_loss1 = 0\n            w_total_loss0 = 0\n            w_total_loss1 = 0\n            start_time = time.time()\n\n            # Attention Maps\n            img_set, _ = \\\n                build_super_images(imgs[-1].cpu(), captions,\n                                   ixtoword, attn_maps, att_sze)\n            if img_set is not None:\n                im = Image.fromarray(img_set)\n                fullpath = '%s/attention_maps%d.png' % (image_dir, step)\n                im.save(fullpath)\n    return count\n\n\ndef evaluate(dataloader, cnn_model, nlp_model, batch_size):\n    cnn_model.eval()\n    nlp_model.eval()    \n    s_total_loss = 0\n    w_total_loss = 0\n    if cfg.DATASET_NAME=='birds':\n        from datasets import TextDataset\n        from datasets import prepare_data\n    if cfg.DATASET_NAME=='flowers':\n        from datasets_flowers import TextDataset\n        from datasets_flowers import prepare_data\n    \n    for step, data in enumerate(dataloader, 0):\n        real_imgs, imgs_2, captions, cap_lens, class_ids, keys, captions_2, cap_lens_2, class_ids_2, \\\n        sort_ind, sort_ind_2 = prepare_data(data)\n\n        words_features, sent_code = cnn_model(real_imgs[-1])\n        # nef = words_features.size(1)\n        # words_features = words_features.view(batch_size, nef, -1)\n\n        words_emb = nlp_model( captions )[0].transpose(1, 2).contiguous()\n        sent_emb = words_emb[ :, :, -1 ].contiguous()\n            # sent_emb = sent_emb.view(batch_size, -1)\n\n        w_loss0, w_loss1, attn = words_loss( words_features, words_emb, labels,\n                                             cap_lens, class_ids, batch_size )\n        w_total_loss += ( w_loss0 + w_loss1 ).data\n\n        s_loss0, s_loss1 = \\\n            sent_loss( sent_code, sent_emb, labels, class_ids, batch_size )\n        s_total_loss += ( s_loss0 + s_loss1 ).data\n\n        if step == 50:\n            break\n\n    s_cur_loss = s_total_loss.item() / step\n    w_cur_loss = w_total_loss.item() / step\n\n    return s_cur_loss, w_cur_loss\n\n\ndef build_models():\n    # build model ############################################################\n    text_encoder = RobertaModel.from_pretrained('roberta-base')\n\n    image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)\n\n    labels = Variable(torch.LongTensor(range(batch_size)))\n    start_epoch = 0\n    if cfg.TRAIN.NET_E:\n        state_dict = torch.load(cfg.TRAIN.NET_E)\n        text_encoder.load_state_dict(state_dict)\n        print('Load ', cfg.TRAIN.NET_E)\n                                                      # output_hidden_states = True )\n          #\n        name = cfg.TRAIN.NET_E.replace( 'text_encoder', 'image_encoder' )\n        state_dict = torch.load(name)\n        image_encoder.load_state_dict(state_dict)\n        print('Load ', name)\n\n        istart = cfg.TRAIN.NET_E.rfind('_') + 8\n        iend = cfg.TRAIN.NET_E.rfind('.')\n        start_epoch = cfg.TRAIN.NET_E[istart:iend]\n        start_epoch = int(start_epoch) + 1\n\n    if cfg.CUDA:\n        text_encoder = text_encoder.cuda()\n        image_encoder = image_encoder.cuda()\n        labels = labels.cuda()\n\n    return text_encoder, image_encoder, labels, start_epoch\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    if args.cfg_file is not None:\n        cfg_from_file(args.cfg_file)\n\n    if args.gpu_id == -1:\n        cfg.CUDA = False\n    else:\n        cfg.GPU_ID = args.gpu_id\n\n    if args.data_dir != '':\n        cfg.DATA_DIR = args.data_dir\n    print('Using config:')\n    pprint.pprint(cfg)\n\n    if not cfg.TRAIN.FLAG:\n        args.manualSeed = 100\n    elif args.manualSeed is None:\n        args.manualSeed = random.randint(1, 10000)\n    random.seed(args.manualSeed)\n    np.random.seed(args.manualSeed)\n    torch.manual_seed(args.manualSeed)\n    if cfg.CUDA:\n        torch.cuda.manual_seed_all(args.manualSeed)\n    if cfg.DATASET_NAME=='birds':\n        from datasets import TextDataset\n        from datasets import prepare_data\n    if cfg.DATASET_NAME=='flowers':\n        from datasets_flowers import TextDataset\n        from datasets_flowers import prepare_data\n\n    ##########################################################################\n    now = datetime.datetime.now(dateutil.tz.tzlocal())\n    timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n    output_dir = 'damsm_output/%s_%s_%s' % \\\n        (cfg.DATASET_NAME, cfg.CONFIG_NAME, timestamp)\n\n    model_dir = os.path.join(output_dir, 'Model')\n    image_dir = os.path.join(output_dir, 'Image')\n    mkdir_p(model_dir)\n    mkdir_p(image_dir)\n\n    torch.cuda.set_device(cfg.GPU_ID)\n    cudnn.benchmark = True\n\n    # Get data loader ##################################################\n    imsize = cfg.TREE.BASE_SIZE * (2 ** (cfg.TREE.BRANCH_NUM-1))\n    batch_size = cfg.TRAIN.BATCH_SIZE\n    image_transform = transforms.Compose([\n        transforms.Resize(int(imsize * 76 / 64)),\n        transforms.RandomCrop(imsize),\n        transforms.RandomHorizontalFlip()])\n    dataset = TextDataset(cfg.DATA_DIR, 'train',\n                          base_size=cfg.TREE.BASE_SIZE,\n                          transform=image_transform)\n\n    print(dataset.n_words, dataset.embeddings_num)\n    assert dataset\n    dataloader = torch.utils.data.DataLoader(\n        dataset, batch_size=batch_size, drop_last=True,\n        shuffle=True, num_workers=int(cfg.WORKERS))\n\n    # # validation data #\n    dataset_val = TextDataset(cfg.DATA_DIR, 'test',\n                              base_size=cfg.TREE.BASE_SIZE,\n                              transform=image_transform)\n    dataloader_val = torch.utils.data.DataLoader(\n        dataset_val, batch_size=batch_size, drop_last=True,\n        shuffle=True, num_workers=int(cfg.WORKERS))\n\n    # Train ##############################################################\n    text_encoder, image_encoder, labels, start_epoch = build_models()\n    para = list(text_encoder.parameters())\n    for v in image_encoder.parameters():\n        if v.requires_grad:\n            para.append(v)\n    # optimizer = optim.Adam(para, lr=cfg.TRAIN.ENCODER_LR, betas=(0.5, 0.999))\n    # At any point you can hit Ctrl + C to break out of training early.\n    try:\n        lr = cfg.TRAIN.ENCODER_LR\n        for epoch in range(start_epoch, cfg.TRAIN.MAX_EPOCH):\n            optimizer = optim.Adam(para, lr=lr, betas=(0.5, 0.999))\n            epoch_start_time = time.time()\n            count = train(dataloader, image_encoder, text_encoder,\n                          batch_size, labels, optimizer, epoch,\n                          dataset.ixtoword, image_dir)\n            print('-' * 89)\n            if len(dataloader_val) > 0:\n                s_loss, w_loss = evaluate(dataloader_val, image_encoder,\n                                          text_encoder, batch_size)\n                print('| end epoch {:3d} | valid loss '\n                      '{:5.5f} {:5.5f} | lr {:.8f}|'\n                      .format(epoch, s_loss, w_loss, lr))\n            print('-' * 89)\n            if lr > cfg.TRAIN.ENCODER_LR/10.:\n                lr *= 0.98\n\n            if (epoch % cfg.TRAIN.SNAPSHOT_INTERVAL == 0 or\n                epoch == cfg.TRAIN.MAX_EPOCH):\n                torch.save(image_encoder.state_dict(),\n                           '%s/image_encoder%d.pth' % (model_dir, epoch))\n                torch.save(text_encoder.state_dict(),\n                           '%s/text_encoder%d.pth' % (model_dir, epoch))\n                print('Save G/Ds models.')\n    except KeyboardInterrupt:\n        print('-' * 89)\n        print('Exiting from training early')","metadata":{"execution":{"iopub.status.busy":"2023-05-11T10:07:39.881248Z","iopub.execute_input":"2023-05-11T10:07:39.881644Z","iopub.status.idle":"2023-05-11T10:07:39.898047Z","shell.execute_reply.started":"2023-05-11T10:07:39.881614Z","shell.execute_reply":"2023-05-11T10:07:39.897020Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Overwriting code/pretrain_DAMSM.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile code/trainer.py\nfrom __future__ import print_function\nfrom six.moves import range\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\n\nfrom PIL import Image\n\nfrom miscc.config import cfg\nfrom miscc.utils import mkdir_p\nfrom miscc.utils import build_super_images, build_super_images2\nfrom miscc.utils import weights_init, load_params, copy_G_params\nfrom model import G_DCGAN, G_NET\nfrom model import RNN_ENCODER, CNN_ENCODER\n\nfrom miscc.losses import words_loss\nfrom miscc.losses import discriminator_loss, generator_loss, KL_loss\nimport os\nimport time\nimport numpy as np\nimport sys\n\nfrom masks import mask_correlated_samples\nfrom nt_xent import NT_Xent\nfrom transformers import RobertaModel\n\nfrom torch.utils.tensorboard import SummaryWriter\n\ndef l2norm(X, dim, eps=1e-8):\n    \"\"\"L2-normalize columns of X\n    \"\"\"\n    norm = torch.pow(X, 2).sum(dim=dim, keepdim=True).sqrt() + eps\n    X = torch.div(X, norm)\n    return X\n\n\n# ################# Text to image task############################ #\nclass condGANTrainer(object):\n    def __init__(self, output_dir, data_loader, n_words, ixtoword, dataset):\n        if cfg.TRAIN.FLAG:\n            self.model_dir = os.path.join(output_dir, 'Model')\n            self.image_dir = os.path.join(output_dir, 'Image')\n            mkdir_p(self.model_dir)\n            mkdir_p(self.image_dir)\n\n        torch.cuda.set_device(cfg.GPU_ID)\n        cudnn.benchmark = True\n\n        self.batch_size = cfg.TRAIN.BATCH_SIZE\n        self.max_epoch = cfg.TRAIN.MAX_EPOCH\n        self.snapshot_interval = cfg.TRAIN.SNAPSHOT_INTERVAL\n\n        self.n_words = n_words\n        self.ixtoword = ixtoword\n        self.data_loader = data_loader\n        self.num_batches = len(self.data_loader)\n        self.dataset = dataset\n        self.writer = SummaryWriter('runs/visualize')\n\n\n    def build_models(self):\n        # ###################encoders######################################## #\n        if cfg.TRAIN.NET_E == '':\n            print('Error: no pretrained text-image encoders')\n            return\n\n        image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)\n        img_encoder_path = cfg.TRAIN.NET_E.replace('text_encoder', 'image_encoder')\n        state_dict = \\\n            torch.load(img_encoder_path, map_location=lambda storage, loc: storage)\n        image_encoder.load_state_dict(state_dict)\n        for p in image_encoder.parameters():\n            p.requires_grad = False\n        print('Load image encoder from:', img_encoder_path)\n        image_encoder.eval()\n\n        text_encoder = RobertaModel.from_pretrained(\"roberta-base\")\n        state_dict = \\\n            torch.load(cfg.TRAIN.NET_E,\n                       map_location=lambda storage, loc: storage)\n        text_encoder.load_state_dict(state_dict)\n        for p in text_encoder.parameters():\n            p.requires_grad = False\n        print('Load text encoder from:', cfg.TRAIN.NET_E)\n        text_encoder.eval()\n\n        # #######################generator and discriminators############## #\n        netsD = []\n        if cfg.GAN.B_DCGAN:\n            if cfg.TREE.BRANCH_NUM ==1:\n                from model import D_NET64 as D_NET\n            elif cfg.TREE.BRANCH_NUM == 2:\n                from model import D_NET128 as D_NET\n            else:  # cfg.TREE.BRANCH_NUM == 3:\n                from model import D_NET256 as D_NET\n            # TODO: elif cfg.TREE.BRANCH_NUM > 3:\n            netG = G_DCGAN()\n            netsD = [D_NET(b_jcu=False)]\n        else:\n            from model import D_NET64, D_NET128, D_NET256\n            netG = G_NET()\n            if cfg.TREE.BRANCH_NUM > 0:\n                netsD.append(D_NET64())\n            if cfg.TREE.BRANCH_NUM > 1:\n                netsD.append(D_NET128())\n            if cfg.TREE.BRANCH_NUM > 2:\n                netsD.append(D_NET256())\n            # TODO: if cfg.TREE.BRANCH_NUM > 3:\n        netG.apply(weights_init)\n        # print(netG)\n        for i in range(len(netsD)):\n            netsD[i].apply(weights_init)\n            # print(netsD[i])\n        print('# of netsD', len(netsD))\n        #\n        epoch = 0\n        if cfg.TRAIN.NET_G != '':\n            state_dict = \\\n                torch.load(cfg.TRAIN.NET_G, map_location=lambda storage, loc: storage)\n            netG.load_state_dict(state_dict)\n            print('Load G from: ', cfg.TRAIN.NET_G)\n            istart = cfg.TRAIN.NET_G.rfind('_') + 1\n            iend = cfg.TRAIN.NET_G.rfind('.')\n            epoch = cfg.TRAIN.NET_G[istart:iend]\n            epoch = int(epoch) + 1\n            if cfg.TRAIN.B_NET_D:\n                Gname = cfg.TRAIN.NET_G\n                for i in range(len(netsD)):\n                    s_tmp = Gname[:Gname.rfind('/')]\n                    Dname = '%s/netD%d.pth' % (s_tmp, i)\n                    print('Load D from: ', Dname)\n                    state_dict = \\\n                        torch.load(Dname, map_location=lambda storage, loc: storage)\n                    netsD[i].load_state_dict(state_dict)\n        # ########################################################### #\n        if cfg.CUDA:\n            text_encoder = text_encoder.cuda()\n            image_encoder = image_encoder.cuda()\n            netG.cuda()\n            for i in range(len(netsD)):\n                netsD[i].cuda()\n        return [text_encoder, image_encoder, netG, netsD, epoch]\n\n    def define_optimizers(self, netG, netsD):\n        optimizersD = []\n        num_Ds = len(netsD)\n        for i in range(num_Ds):\n            opt = optim.Adam(netsD[i].parameters(),\n                             lr=cfg.TRAIN.DISCRIMINATOR_LR,\n                             betas=(0.5, 0.999))\n            optimizersD.append(opt)\n\n        optimizerG = optim.Adam(netG.parameters(),\n                                lr=cfg.TRAIN.GENERATOR_LR,\n                                betas=(0.5, 0.999))\n\n        return optimizerG, optimizersD\n\n    def prepare_labels(self):\n        batch_size = self.batch_size\n        real_labels = Variable(torch.FloatTensor(batch_size).fill_(1))\n        fake_labels = Variable(torch.FloatTensor(batch_size).fill_(0))\n        match_labels = Variable(torch.LongTensor(range(batch_size)))\n        if cfg.CUDA:\n            real_labels = real_labels.cuda()\n            fake_labels = fake_labels.cuda()\n            match_labels = match_labels.cuda()\n\n        return real_labels, fake_labels, match_labels\n\n    def save_model(self, netG, avg_param_G, netsD, epoch):\n        backup_para = copy_G_params(netG)\n        load_params(netG, avg_param_G)\n        torch.save(netG.state_dict(),\n            '%s/netG_epoch_%d.pth' % (self.model_dir, epoch))\n        load_params(netG, backup_para)\n        #\n        for i in range(len(netsD)):\n            netD = netsD[i]\n            torch.save(netD.state_dict(),\n                '%s/netD%d.pth' % (self.model_dir, i))\n        print('Save G/Ds models.')\n\n    def set_requires_grad_value(self, models_list, brequires):\n        for i in range(len(models_list)):\n            for p in models_list[i].parameters():\n                p.requires_grad = brequires\n\n    def save_img_results(self, netG, noise, sent_emb, words_embs, mask,\n                         image_encoder, captions, cap_lens,\n                         gen_iterations, name='current'):\n        # Save images\n        fake_imgs, attention_maps, _, _ = netG(noise, sent_emb, words_embs, mask)\n        for i in range(len(attention_maps)):\n            if len(fake_imgs) > 1:\n                img = fake_imgs[i + 1].detach().cpu()\n                lr_img = fake_imgs[i].detach().cpu()\n            else:\n                img = fake_imgs[0].detach().cpu()\n                lr_img = None\n            attn_maps = attention_maps[i]\n            att_sze = attn_maps.size(2)\n            img_set, _ = \\\n                build_super_images(img, captions, self.ixtoword,\n                                   attn_maps, att_sze, lr_imgs=lr_img)\n            if img_set is not None:\n                im = Image.fromarray(img_set)\n                fullpath = '%s/G_%s_%d_%d.png'\\\n                    % (self.image_dir, name, gen_iterations, i)\n                im.save(fullpath)\n\n        # for i in range(len(netsD)):\n        i = -1\n        img = fake_imgs[i].detach()\n        region_features, _ = image_encoder(img)\n        att_sze = region_features.size(2)\n        _, _, att_maps = words_loss(region_features.detach(),\n                                    words_embs.detach(),\n                                    None, cap_lens,\n                                    None, self.batch_size)\n        img_set, _ = \\\n            build_super_images(fake_imgs[i].detach().cpu(),\n                               captions, self.ixtoword, att_maps, att_sze)\n        if img_set is not None:\n            im = Image.fromarray(img_set)\n            fullpath = '%s/D_%s_%d.png'\\\n                % (self.image_dir, name, gen_iterations)\n            im.save(fullpath)\n\n    def train(self):\n        text_encoder, image_encoder, netG, netsD, start_epoch = self.build_models()\n        avg_param_G = copy_G_params(netG)\n        optimizerG, optimizersD = self.define_optimizers(netG, netsD)\n        real_labels, fake_labels, match_labels = self.prepare_labels()\n\n        real_labels_2, fake_labels_2, match_labels_2 = self.prepare_labels()\n\n        batch_size = self.batch_size\n        nz = cfg.GAN.Z_DIM\n        noise = Variable(torch.FloatTensor(batch_size, nz))\n        fixed_noise = Variable(torch.FloatTensor(batch_size, nz).normal_(0, 1))\n        if cfg.CUDA:\n            noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n\n        gen_iterations = 0\n\n        mask = mask_correlated_samples(self)\n\n        temperature = 0.5\n        device = noise.get_device()\n        criterion = NT_Xent(batch_size, temperature, mask, device)\n        if cfg.DATASET_NAME == 'birds':\n            from datasets import prepare_data\n        if cfg.DATASET_NAME == 'flowers':\n            from datasets_flowers import prepare_data\n\n        # gen_iterations = start_epoch * self.num_batches\n        for epoch in range(start_epoch, self.max_epoch):\n            start_t = time.time()\n\n            data_iter = iter(self.data_loader)\n            step = 0\n\n            D_total_loss = 0\n            G_total_loss = 0\n\n            while step < self.num_batches:\n                # reset requires_grad to be trainable for all Ds\n                # self.set_requires_grad_value(netsD, True)\n\n                ######################################################\n                # (1) Prepare training data and Compute text embeddings\n                ######################################################\n                data = next(data_iter)\n                imgs, imgs_2, captions, cap_lens, class_ids, keys, captions_2, cap_lens_2, class_ids_2, \\\n                sort_ind, sort_ind_2 = prepare_data(data)\n\n                words_embs = text_encoder( captions )[0].transpose(1, 2).contiguous()\n                    #words_embs = torch.Tensor(words_embs).cuda()\n                sent_emb = words_embs[ :, :, -1 ].contiguous()\n                words_embs, sent_emb = words_embs.detach(), sent_emb.detach()\n                mask = (captions == 0)\n                num_words = words_embs.size(2)\n                if mask.size(1) > num_words:\n                    mask = mask[:, :num_words]\n\n                words_embs_2 = text_encoder( captions_2 )[0].transpose(1, 2).contiguous()\n                    #words_embs = torch.Tensor(words_embs).cuda()\n                sent_emb_2 = words_embs_2[ :, :, -1 ].contiguous()\n                words_embs_2, sent_emb_2 = words_embs_2.detach(), sent_emb_2.detach()\n                mask_2 = (captions_2 == 0)\n                num_words_2 = words_embs_2.size(2)\n                if mask_2.size(1) > num_words_2:\n                    mask_2 = mask_2[:, :num_words_2]\n\n                #######################################################\n                # (2) Generate fake images\n                ######################################################\n                noise.data.normal_(0, 1)\n                fake_imgs, _, mu, logvar = netG(noise, sent_emb, words_embs, mask)\n                fake_imgs_2, _, mu_2, logvar_2 = netG(noise, sent_emb_2, words_embs_2, mask_2)\n\n                #######################################################\n                # (3) Update D network\n                ######################################################\n                errD_total = 0\n                D_logs = ''\n                for i in range(len(netsD)):\n                    netsD[i].zero_grad()\n                    errD = discriminator_loss(netsD[i], imgs[i], fake_imgs[i],\n                                              sent_emb, real_labels, fake_labels)\n                    errD_2 = discriminator_loss(netsD[i], imgs_2[i], fake_imgs_2[i],\n                                                sent_emb_2, real_labels_2, fake_labels_2)\n                    errD += errD_2\n\n                    # backward and update parameters\n                    errD.backward()\n                    optimizersD[i].step()\n                    errD_total += errD\n                    D_logs += 'errD%d: %.2f ' % (i, errD.item())\n\n                #######################################################\n                # (4) Update G network: maximize log(D(G(z)))\n                ######################################################\n                # compute total loss for training G\n                step += 1\n                gen_iterations += 1\n\n                # do not need to compute gradient for Ds\n                # self.set_requires_grad_value(netsD, False)\n                netG.zero_grad()\n                errG_total, G_logs, cnn_code = \\\n                    generator_loss(netsD, image_encoder, fake_imgs, real_labels,\n                                   words_embs, sent_emb, match_labels, cap_lens, class_ids)\n                kl_loss = KL_loss(mu, logvar)\n                errG_total += kl_loss\n                G_logs += 'kl_loss: %.2f ' % kl_loss.item()\n\n                errG_total_2, G_logs_2, cnn_code_2 = \\\n                    generator_loss(netsD, image_encoder, fake_imgs_2, real_labels_2,\n                                   words_embs_2, sent_emb_2, match_labels_2, cap_lens_2, class_ids_2)\n                kl_loss_2 = KL_loss(mu_2, logvar_2)\n                errG_total_2 += kl_loss_2\n                G_logs_2 += 'kl_loss: %.2f ' % kl_loss_2.item()\n\n                errG_total += errG_total_2\n\n                _, ori_indices = torch.sort(sort_ind, 0)\n                _, ori_indices_2 = torch.sort(sort_ind_2, 0)\n\n                total_contra_loss = 0\n                i = -1\n                cnn_code = cnn_code[ori_indices]\n                cnn_code_2 = cnn_code_2[ori_indices_2]\n\n                cnn_code = l2norm(cnn_code, dim=1)\n                cnn_code_2 = l2norm(cnn_code_2, dim=1)\n\n                contrative_loss = criterion(cnn_code, cnn_code_2)\n                total_contra_loss += contrative_loss *  0.2\n                G_logs += 'contrative_loss: %.2f ' % total_contra_loss.item()\n                errG_total += total_contra_loss\n                # backward and update parameters\n                errG_total.backward()\n                optimizerG.step()\n                for p, avg_p in zip(netG.parameters(), avg_param_G):\n                    avg_p.mul_(0.999).add_(0.001, p.data)\n\n                if gen_iterations % 100 == 0:\n                    print(D_logs + '\\n' + G_logs + '\\n' + G_logs_2)\n                # save images\n                if gen_iterations % 1000 == 0:\n                    backup_para = copy_G_params(netG)\n                    load_params(netG, avg_param_G)\n                    self.save_img_results(netG, fixed_noise, sent_emb,\n                                           words_embs, mask, image_encoder,\n                                           captions, cap_lens, epoch, name='average')\n                    load_params(netG, backup_para)\n                    #\n                    self.save_img_results(netG, fixed_noise, sent_emb,\n                                           words_embs, mask, image_encoder,\n                                           captions, cap_lens,\n                                           epoch, name='current')\n                D_total_loss += errD_total.item()\n                G_total_loss += errG_total.item()\n\n            end_t = time.time()\n\n            print('''[%d/%d][%d]\n                  Loss_D: %.2f Loss_G: %.2f Time: %.2fs'''\n                  % (epoch, self.max_epoch, self.num_batches,\n                     errD_total.item(), errG_total.item(),\n                     end_t - start_t))\n\n            if epoch % cfg.TRAIN.SNAPSHOT_INTERVAL == 0:  # and epoch != 0:\n                self.save_model(netG, avg_param_G, netsD, epoch)\n\n            D_total_loss = D_total_loss / step\n            G_total_loss = G_total_loss / step\n            # self.writer.add_scalar('Loss_D', D_total_loss , epoch  + 1)\n            # self.writer.add_scalar('Loss_G', G_total_loss , epoch  + 1)\n            self.writer.add_scalars('Loss_D and Loss_G', {'Loss_D': D_total_loss, 'Loss_G': G_total_loss}, epoch  + 1)\n\n        self.writer.close()\n\n        self.save_model(netG, avg_param_G, netsD, self.max_epoch)\n\n    def save_singleimages(self, images, filenames, save_dir,\n                          split_dir, sentenceID=0):\n        for i in range(images.size(0)):\n            s_tmp = '%s/single_samples/%s/%s' %\\\n                (save_dir, split_dir, filenames[i])\n            folder = s_tmp[:s_tmp.rfind('/')]\n            if not os.path.isdir(folder):\n                print('Make a new folder: ', folder)\n                mkdir_p(folder)\n\n            fullpath = '%s_%d.jpg' % (s_tmp, sentenceID)\n            # range from [-1, 1] to [0, 1]\n            # img = (images[i] + 1.0) / 2\n            img = images[i].add(1).div(2).mul(255).clamp(0, 255).byte()\n            # range from [0, 1] to [0, 255]\n            ndarr = img.permute(1, 2, 0).data.cpu().numpy()\n            im = Image.fromarray(ndarr)\n            im.save(fullpath)\n\n    def sampling(self, split_dir):\n        if cfg.DATASET_NAME=='birds':\n            from datasets import prepare_data\n        if cfg.DATASET_NAME=='flowers':\n            from datasets_flowers import prepare_data\n        if cfg.TRAIN.NET_G == '':\n            print('Error: the path for morels is not found!')\n        else:\n            if split_dir == 'test':\n                split_dir = 'valid'\n            # Build and load the generator\n            if cfg.GAN.B_DCGAN:\n                netG = G_DCGAN()\n            else:\n                netG = G_NET()\n            netG.apply(weights_init)\n            netG.cuda()\n            netG.eval()\n\n            # load text encoder\n            text_encoder = RobertaModel.from_pretrained(\"roberta-base\")\n            state_dict = torch.load(cfg.TRAIN.NET_E, map_location=lambda storage, loc: storage)\n            text_encoder.load_state_dict(state_dict)\n            print('Load text encoder from:', cfg.TRAIN.NET_E)\n            text_encoder = text_encoder.cuda()\n            text_encoder.eval()\n\n            #load image encoder\n            image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)\n            img_encoder_path = cfg.TRAIN.NET_E.replace('text_encoder', 'image_encoder')\n            state_dict = torch.load(img_encoder_path, map_location=lambda storage, loc: storage)\n            image_encoder.load_state_dict(state_dict)\n            print('Load image encoder from:', img_encoder_path)\n            image_encoder = image_encoder.cuda()\n            image_encoder.eval()\n\n            batch_size = self.batch_size\n            nz = cfg.GAN.Z_DIM\n            noise = Variable(torch.FloatTensor(batch_size, nz), volatile=True)\n            noise = noise.cuda()\n\n            model_dir = cfg.TRAIN.NET_G\n            state_dict = torch.load(model_dir, map_location=lambda storage, loc: storage)\n            # state_dict = torch.load(cfg.TRAIN.NET_G)\n            netG.load_state_dict(state_dict)\n            print('Load G from: ', model_dir)\n\n            # the path to save generated images\n            s_tmp = 'val_gen_images'\n            save_dir = '%s/%s' % (s_tmp, split_dir)\n            mkdir_p(save_dir)\n\n            cnt = 0\n            R_count = 0\n            R = np.zeros(30000)\n            cont = True\n            for ii in range(11):  # (cfg.TEXT.CAPTIONS_PER_IMAGE):\n                if (cont == False):\n                    break\n                for step, data in enumerate(self.data_loader, 0):\n                    cnt += batch_size\n                    if (cont == False):\n                        break\n                    if step % 100 == 0:\n                       print('cnt: ', cnt)\n                    # if step > 50:\n                    #     break\n\n                    # imgs, captions, cap_lens, class_ids, keys = prepare_data(data)\n\n                    imgs, imgs_2, captions, cap_lens, class_ids, keys, captions_2, cap_lens_2, class_ids_2, \\\n                    sort_ind, sort_ind_2 = prepare_data(data)\n\n                    words_embs = text_encoder( captions )[0].transpose(1, 2).contiguous()\n                    sent_emb = words_embs[ :, :, -1 ].contiguous()\n                    words_embs, sent_emb = words_embs.detach(), sent_emb.detach()\n                    mask = (captions == 0)\n                    num_words = words_embs.size(2)\n                    if mask.size(1) > num_words:\n                        mask = mask[:, :num_words]\n\n                    #######################################################\n                    # (2) Generate fake images\n                    ######################################################\n                    noise.data.normal_(0, 1)\n                    fake_imgs, _, _, _ = netG(noise, sent_emb, words_embs, mask)\n                    for j in range(batch_size):\n                        s_tmp = '%s/single/%s' % (save_dir, keys[j])\n                        folder = s_tmp[:s_tmp.rfind('/')]\n                        if not os.path.isdir(folder):\n                            #print('Make a new folder: ', folder)\n                            mkdir_p(folder)\n                        k = -1\n                        # for k in range(len(fake_imgs)):\n                        im = fake_imgs[k][j].data.cpu().numpy()\n                        # [-1, 1] --> [0, 255]\n                        im = (im + 1.0) * 127.5\n                        im = im.astype(np.uint8)\n                        im = np.transpose(im, (1, 2, 0))\n                        im = Image.fromarray(im)\n                        fullpath = '%s_s%d_%d.png' % (s_tmp, k, ii)\n                        im.save(fullpath)\n\n                    _, cnn_code = image_encoder(fake_imgs[-1])\n\n                    for i in range(batch_size):\n                        mis_captions, mis_captions_len = self.dataset.get_mis_caption(class_ids[i])\n                        words_embs_t = text_encoder(mis_captions )[0].transpose(1, 2).contiguous()\n                        sent_emb_t = words_embs[ :, :, -1 ].contiguous()\n                        rnn_code = torch.cat((sent_emb[i, :].unsqueeze(0), sent_emb_t), 0)\n                        ### cnn_code = 1 * nef\n                        ### rnn_code = 100 * nef\n                        scores = torch.mm(cnn_code[i].unsqueeze(0), rnn_code.transpose(0, 1))  # 1* 100\n                        cnn_code_norm = torch.norm(cnn_code[i].unsqueeze(0), 2, dim=1, keepdim=True)\n                        rnn_code_norm = torch.norm(rnn_code, 2, dim=1, keepdim=True)\n                        norm = torch.mm(cnn_code_norm, rnn_code_norm.transpose(0, 1))\n                        scores0 = scores / norm.clamp(min=1e-8)\n                        if torch.argmax(scores0) == 0:\n                            R[R_count] = 1\n                        R_count += 1\n\n                    if R_count >= 30000:\n                        sum = np.zeros(10)\n                        np.random.shuffle(R)\n                        for i in range(10):\n                            sum[i] = np.average(R[i * 3000:(i + 1) * 3000 - 1])\n                        R_mean = np.average(sum)\n                        R_std = np.std(sum)\n                        print(\"R mean:{:.4f} std:{:.4f}\".format(R_mean, R_std))\n                        cont = False\n\n    def gen_example(self, data_dic):\n        if cfg.TRAIN.NET_G == '':\n            print('Error: the path for morels is not found!')\n        else:\n            # Build and load the generator\n            text_encoder = RobertaModel.from_pretrained(\"roberta-base\")\n            state_dict = \\\n                torch.load(cfg.TRAIN.NET_E, map_location=lambda storage, loc: storage)\n            text_encoder.load_state_dict(state_dict)\n            print('Load text encoder from:', cfg.TRAIN.NET_E)\n            text_encoder = text_encoder.cuda()\n            text_encoder.eval()\n\n            # the path to save generated images\n            if cfg.GAN.B_DCGAN:\n                netG = G_DCGAN()\n            else:\n                netG = G_NET()\n            s_tmp = 'example_images'\n            model_dir = cfg.TRAIN.NET_G\n            state_dict = \\\n                torch.load(model_dir, map_location=lambda storage, loc: storage)\n            netG.load_state_dict(state_dict)\n            print('Load G from: ', model_dir)\n            netG.cuda()\n            netG.eval()\n            for key in data_dic:\n                save_dir = '%s/%s' % (s_tmp, key)\n                mkdir_p(save_dir)\n                captions, cap_lens, sorted_indices = data_dic[key]\n\n                batch_size = captions.shape[0]\n                nz = cfg.GAN.Z_DIM\n                captions = Variable(torch.from_numpy(captions), volatile=True)\n                cap_lens = Variable(torch.from_numpy(cap_lens), volatile=True)\n\n                captions = captions.cuda()\n                cap_lens = cap_lens.cuda()\n                for i in range(1):  # 16\n                    noise = Variable(torch.FloatTensor(batch_size, nz), volatile=True)\n                    noise = noise.cuda()\n                    #######################################################\n                    # (1) Extract text embeddings\n                    ######################################################\n                    words_embs = text_encoder( captions )[0].transpose(1, 2).contiguous()\n                    sent_emb = words_embs[ :, :, -1 ].contiguous()\n                    mask = (captions == 0)\n                    #######################################################\n                    # (2) Generate fake images\n                    ######################################################\n                    noise.data.normal_(0, 1)\n                    fake_imgs, attention_maps, _, _ = netG(noise, sent_emb, words_embs, mask)\n                    # G attention\n                    cap_lens_np = cap_lens.cpu().data.numpy()\n                    for j in range(batch_size):\n                        save_name = '%s/%d_s_%d' % (save_dir, i, sorted_indices[j])\n                        for k in range(len(fake_imgs)):\n                            im = fake_imgs[k][j].data.cpu().numpy()\n                            im = (im + 1.0) * 127.5\n                            im = im.astype(np.uint8)\n                            # print('im', im.shape)\n                            im = np.transpose(im, (1, 2, 0))\n                            # print('im', im.shape)\n                            im = Image.fromarray(im)\n                            fullpath = '%s_g%d.png' % (save_name, k)\n                            im.save(fullpath)\n\n                        for k in range(len(attention_maps)):\n                            if len(fake_imgs) > 1:\n                                im = fake_imgs[k + 1].detach().cpu()\n                            else:\n                                im = fake_imgs[0].detach().cpu()\n                            attn_maps = attention_maps[k]\n                            att_sze = attn_maps.size(2)\n                            img_set, sentences = \\\n                                build_super_images2(im[j].unsqueeze(0),\n                                                    captions[j].unsqueeze(0),\n                                                    [cap_lens_np[j]], self.ixtoword,\n                                                    [attn_maps[j]], att_sze)\n                            if img_set is not None:\n                                im = Image.fromarray(img_set)\n                                fullpath = '%s_a%d.png' % (save_name, k)\n                        ","metadata":{"execution":{"iopub.status.busy":"2023-05-12T05:36:47.611150Z","iopub.execute_input":"2023-05-12T05:36:47.611576Z","iopub.status.idle":"2023-05-12T05:36:47.639180Z","shell.execute_reply.started":"2023-05-12T05:36:47.611537Z","shell.execute_reply":"2023-05-12T05:36:47.637792Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Overwriting code/trainer.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Download the checkpoints of DAMSM phase and GAN training phase from here: https://drive.google.com/drive/folders/1f6yRM3saZJvBBdmado7ACA_8JsL8vTIg?usp=share_link","metadata":{}},{"cell_type":"markdown","source":"**Experiment 2 (RoBERTa transformer + SN + CL (GAN Training))**","metadata":{}},{"cell_type":"markdown","source":"1. Clone the repository\n2. Make the following changes in pretrain_damsm.py files.\n3. Follow the same procedure for DAMSM training, GAN training and evaluation.","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/mansivv9/Text_to_image_synthesis_Major_Project.git","metadata":{"execution":{"iopub.status.busy":"2023-05-12T06:06:00.607476Z","iopub.execute_input":"2023-05-12T06:06:00.607867Z","iopub.status.idle":"2023-05-12T06:06:02.604961Z","shell.execute_reply.started":"2023-05-12T06:06:00.607835Z","shell.execute_reply":"2023-05-12T06:06:02.603740Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'Text_to_image_synthesis_Major_Project'...\nremote: Enumerating objects: 206, done.\u001b[K\nremote: Counting objects: 100% (120/120), done.\u001b[K\nremote: Compressing objects: 100% (98/98), done.\u001b[K\nremote: Total 206 (delta 44), reused 69 (delta 22), pack-reused 86\u001b[K\nReceiving objects: 100% (206/206), 491.48 KiB | 4.96 MiB/s, done.\nResolving deltas: 100% (67/67), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.chdir('/kaggle/working/Text_to_image_synthesis_Major_Project/AttnGAN+CL+SN+RoBERTa(AttnGAN_V2)')","metadata":{"execution":{"iopub.status.busy":"2023-05-12T06:06:04.449832Z","iopub.execute_input":"2023-05-12T06:06:04.450260Z","iopub.status.idle":"2023-05-12T06:06:04.456808Z","shell.execute_reply.started":"2023-05-12T06:06:04.450199Z","shell.execute_reply":"2023-05-12T06:06:04.455809Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"%%writefile code/pretrain_DAMSM.py\nfrom __future__ import print_function\n\nfrom miscc.utils import mkdir_p\nfrom miscc.utils import build_super_images\nfrom miscc.losses import sent_loss, words_loss\nfrom miscc.config import cfg, cfg_from_file\n\nfrom model import RNN_ENCODER, CNN_ENCODER\n\nimport os\nimport sys\nimport time\nimport random\nimport pprint\nimport datetime\nimport dateutil.tz\nimport argparse\nimport numpy as np\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as transforms\n# from torch.nn.utils.rnn import pad_packed_sequence\n\nfrom transformers import RobertaModel\n\n\ndir_path = (os.path.abspath(os.path.join(os.path.realpath(__file__), './.')))\nsys.path.append(dir_path)\n\n\nUPDATE_INTERVAL = 50\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Train a DAMSM network')\n    parser.add_argument('--cfg', dest='cfg_file',\n                        help='optional config file',\n                        default='cfg/DAMSM/bird.yml', type=str)\n    parser.add_argument('--gpu', dest='gpu_id', type=int, default=0)\n    parser.add_argument('--data_dir', dest='data_dir', type=str, default='')\n    parser.add_argument('--manualSeed', type=int, help='manual seed')\n    args = parser.parse_args()\n    return args\n\n\ndef train( dataloader, cnn_model, nlp_model, batch_size,\n           labels, optimizer, epoch, ixtoword, image_dir ):\n    \n    if cfg.DATASET_NAME=='birds':\n        from datasets import TextDataset\n        from datasets import prepare_data\n    if cfg.DATASET_NAME=='flowers':\n        from datasets_flowers import TextDataset\n        from datasets_flowers import prepare_data\n        \n    cnn_model.train()\n    nlp_model.train()\n    s_total_loss0 = 0\n    s_total_loss1 = 0\n    w_total_loss0 = 0\n    w_total_loss1 = 0\n    count = (epoch + 1) * len(dataloader)\n    start_time = time.time()\n    for step, data in enumerate(dataloader, 0):\n        # print('step', step)\n        nlp_model.zero_grad()\n        cnn_model.zero_grad()\n\n        imgs, imgs_2, captions, cap_lens, class_ids, keys, captions_2, cap_lens_2, class_ids_2, \\\n        sort_ind, sort_ind_2 = prepare_data(data)\n\n\n        # words_features: batch_size x nef x 17 x 17\n        # sent_code: batch_size x nef\n        words_features, sent_code = cnn_model(imgs[-1])\n        # print( words_features.shape, sent_code.shape )\n        # --> batch_size x nef x 17*17\n        nef, att_sze = words_features.size(1), words_features.size(2)\n        # words_features = words_features.view(batch_size, nef, -1)\n\n        # Forward Prop:\n        # inputs:\n        #   captions: torch.LongTensor of ids of size batch x n_steps\n        # outputs:\n        #   words_emb: batch_size x nef x seq_len\n        #   sent_emb: batch_size x nef\n        \n        words_emb = nlp_model( captions )[0].transpose(1, 2).contiguous()\n        sent_emb = words_emb[ :, :, -1 ].contiguous()\n            # sent_emb = sent_emb.view(batch_size, -1)\n        # print( words_emb.shape, sent_emb.shape )\n\n        # Compute Loss:\n        # NOTE: the ideal loss for Transformer may be different than that for bi-directional LSTM\n        w_loss0, w_loss1, attn_maps = words_loss( words_features, words_emb, labels,\n                                                  cap_lens, class_ids, batch_size )\n        w_total_loss0 += w_loss0.data\n        w_total_loss1 += w_loss1.data\n        loss = w_loss0 + w_loss1\n\n        s_loss0, s_loss1 = \\\n            sent_loss( sent_code, sent_emb, labels, class_ids, batch_size )\n        loss += s_loss0 + s_loss1\n        s_total_loss0 += s_loss0.data\n        s_total_loss1 += s_loss1.data\n        #\n        # Backprop:\n        loss.backward()\n        #\n        # `clip_grad_norm` helps prevent\n        # the exploding gradient problem in RNNs / LSTMs.\n        optimizer.step()\n\n        if step % UPDATE_INTERVAL == 0:\n            count = epoch * len(dataloader) + step\n\n            # print(  s_total_loss0, s_total_loss1 )\n            s_cur_loss0 = s_total_loss0.item() / UPDATE_INTERVAL\n            s_cur_loss1 = s_total_loss1.item() / UPDATE_INTERVAL\n\n            # print(  w_total_loss0, w_total_loss1 )\n            w_cur_loss0 = w_total_loss0.item() / UPDATE_INTERVAL\n            w_cur_loss1 = w_total_loss1.item() / UPDATE_INTERVAL\n\n            elapsed = time.time() - start_time\n            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n                  's_loss {:5.5f} {:5.5f} | '\n                  'w_loss {:5.5f} {:5.5f}'\n                  .format(epoch, step, len(dataloader),\n                          elapsed * 1000. / UPDATE_INTERVAL,\n                          s_cur_loss0, s_cur_loss1,\n                          w_cur_loss0, w_cur_loss1))\n            s_total_loss0 = 0\n            s_total_loss1 = 0\n            w_total_loss0 = 0\n            w_total_loss1 = 0\n            start_time = time.time()\n\n            # Attention Maps\n            img_set, _ = \\\n                build_super_images(imgs[-1].cpu(), captions,\n                                   ixtoword, attn_maps, att_sze)\n            if img_set is not None:\n                im = Image.fromarray(img_set)\n                fullpath = '%s/attention_maps%d.png' % (image_dir, step)\n                im.save(fullpath)\n    return count\n\n\ndef evaluate(dataloader, cnn_model, nlp_model, batch_size):\n    cnn_model.eval()\n    nlp_model.eval()    \n    s_total_loss = 0\n    w_total_loss = 0\n    if cfg.DATASET_NAME=='birds':\n        from datasets import TextDataset\n        from datasets import prepare_data\n    if cfg.DATASET_NAME=='flowers':\n        from datasets_flowers import TextDataset\n        from datasets_flowers import prepare_data\n    \n    for step, data in enumerate(dataloader, 0):\n        real_imgs, imgs_2, captions, cap_lens, class_ids, keys, captions_2, cap_lens_2, class_ids_2, \\\n        sort_ind, sort_ind_2 = prepare_data(data)\n\n        words_features, sent_code = cnn_model(real_imgs[-1])\n        # nef = words_features.size(1)\n        # words_features = words_features.view(batch_size, nef, -1)\n\n        words_emb = nlp_model( captions )[0].transpose(1, 2).contiguous()\n        sent_emb = words_emb[ :, :, -1 ].contiguous()\n            # sent_emb = sent_emb.view(batch_size, -1)\n\n        w_loss0, w_loss1, attn = words_loss( words_features, words_emb, labels,\n                                             cap_lens, class_ids, batch_size )\n        w_total_loss += ( w_loss0 + w_loss1 ).data\n\n        s_loss0, s_loss1 = \\\n            sent_loss( sent_code, sent_emb, labels, class_ids, batch_size )\n        s_total_loss += ( s_loss0 + s_loss1 ).data\n\n        if step == 50:\n            break\n\n    s_cur_loss = s_total_loss.item() / step\n    w_cur_loss = w_total_loss.item() / step\n\n    return s_cur_loss, w_cur_loss\n\n\ndef build_models():\n    # build model ############################################################\n    text_encoder = RobertaModel.from_pretrained('roberta-base')\n\n    image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)\n\n    labels = Variable(torch.LongTensor(range(batch_size)))\n    start_epoch = 0\n    if cfg.TRAIN.NET_E:\n        state_dict = torch.load(cfg.TRAIN.NET_E)\n        text_encoder.load_state_dict(state_dict)\n        print('Load ', cfg.TRAIN.NET_E)\n                                                      # output_hidden_states = True )\n          #\n        name = cfg.TRAIN.NET_E.replace( 'text_encoder', 'image_encoder' )\n        state_dict = torch.load(name)\n        image_encoder.load_state_dict(state_dict)\n        print('Load ', name)\n\n        istart = cfg.TRAIN.NET_E.rfind('_') + 8\n        iend = cfg.TRAIN.NET_E.rfind('.')\n        start_epoch = cfg.TRAIN.NET_E[istart:iend]\n        start_epoch = int(start_epoch) + 1\n\n    if cfg.CUDA:\n        text_encoder = text_encoder.cuda()\n        image_encoder = image_encoder.cuda()\n        labels = labels.cuda()\n\n    return text_encoder, image_encoder, labels, start_epoch\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    if args.cfg_file is not None:\n        cfg_from_file(args.cfg_file)\n\n    if args.gpu_id == -1:\n        cfg.CUDA = False\n    else:\n        cfg.GPU_ID = args.gpu_id\n\n    if args.data_dir != '':\n        cfg.DATA_DIR = args.data_dir\n    print('Using config:')\n    pprint.pprint(cfg)\n\n    if not cfg.TRAIN.FLAG:\n        args.manualSeed = 100\n    elif args.manualSeed is None:\n        args.manualSeed = random.randint(1, 10000)\n    random.seed(args.manualSeed)\n    np.random.seed(args.manualSeed)\n    torch.manual_seed(args.manualSeed)\n    if cfg.CUDA:\n        torch.cuda.manual_seed_all(args.manualSeed)\n    if cfg.DATASET_NAME=='birds':\n        from datasets import TextDataset\n        from datasets import prepare_data\n    if cfg.DATASET_NAME=='flowers':\n        from datasets_flowers import TextDataset\n        from datasets_flowers import prepare_data\n\n    ##########################################################################\n    now = datetime.datetime.now(dateutil.tz.tzlocal())\n    timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n    output_dir = 'damsm_output/%s_%s_%s' % \\\n        (cfg.DATASET_NAME, cfg.CONFIG_NAME, timestamp)\n\n    model_dir = os.path.join(output_dir, 'Model')\n    image_dir = os.path.join(output_dir, 'Image')\n    mkdir_p(model_dir)\n    mkdir_p(image_dir)\n\n    torch.cuda.set_device(cfg.GPU_ID)\n    cudnn.benchmark = True\n\n    # Get data loader ##################################################\n    imsize = cfg.TREE.BASE_SIZE * (2 ** (cfg.TREE.BRANCH_NUM-1))\n    batch_size = cfg.TRAIN.BATCH_SIZE\n    image_transform = transforms.Compose([\n        transforms.Resize(int(imsize * 76 / 64)),\n        transforms.RandomCrop(imsize),\n        transforms.RandomHorizontalFlip()])\n    dataset = TextDataset(cfg.DATA_DIR, 'train',\n                          base_size=cfg.TREE.BASE_SIZE,\n                          transform=image_transform)\n\n    print(dataset.n_words, dataset.embeddings_num)\n    assert dataset\n    dataloader = torch.utils.data.DataLoader(\n        dataset, batch_size=batch_size, drop_last=True,\n        shuffle=True, num_workers=int(cfg.WORKERS))\n\n    # # validation data #\n    dataset_val = TextDataset(cfg.DATA_DIR, 'test',\n                              base_size=cfg.TREE.BASE_SIZE,\n                              transform=image_transform)\n    dataloader_val = torch.utils.data.DataLoader(\n        dataset_val, batch_size=batch_size, drop_last=True,\n        shuffle=True, num_workers=int(cfg.WORKERS))\n\n    # Train ##############################################################\n    text_encoder, image_encoder, labels, start_epoch = build_models()\n    para = list(text_encoder.parameters())\n    for v in image_encoder.parameters():\n        if v.requires_grad:\n            para.append(v)\n    # optimizer = optim.Adam(para, lr=cfg.TRAIN.ENCODER_LR, betas=(0.5, 0.999))\n    # At any point you can hit Ctrl + C to break out of training early.\n    try:\n        lr = cfg.TRAIN.ENCODER_LR\n        for epoch in range(start_epoch, cfg.TRAIN.MAX_EPOCH):\n            optimizer = optim.Adam(para, lr=lr, betas=(0.5, 0.999))\n            epoch_start_time = time.time()\n            count = train(dataloader, image_encoder, text_encoder,\n                          batch_size, labels, optimizer, epoch,\n                          dataset.ixtoword, image_dir)\n            print('-' * 89)\n            if len(dataloader_val) > 0:\n                s_loss, w_loss = evaluate(dataloader_val, image_encoder,\n                                          text_encoder, batch_size)\n                print('| end epoch {:3d} | valid loss '\n                      '{:5.5f} {:5.5f} | lr {:.8f}|'\n                      .format(epoch, s_loss, w_loss, lr))\n            print('-' * 89)\n            if lr > cfg.TRAIN.ENCODER_LR/10.:\n                lr *= 0.98\n\n            if (epoch % cfg.TRAIN.SNAPSHOT_INTERVAL == 0 or\n                epoch == cfg.TRAIN.MAX_EPOCH):\n                torch.save(image_encoder.state_dict(),\n                           '%s/image_encoder%d.pth' % (model_dir, epoch))\n                torch.save(text_encoder.state_dict(),\n                           '%s/text_encoder%d.pth' % (model_dir, epoch))\n                print('Save G/Ds models.')\n    except KeyboardInterrupt:\n        print('-' * 89)\n        print('Exiting from training early')","metadata":{"execution":{"iopub.status.busy":"2023-05-12T06:07:54.501898Z","iopub.execute_input":"2023-05-12T06:07:54.502288Z","iopub.status.idle":"2023-05-12T06:07:54.519947Z","shell.execute_reply.started":"2023-05-12T06:07:54.502246Z","shell.execute_reply":"2023-05-12T06:07:54.518896Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Overwriting code/pretrain_DAMSM.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Download the checkpoints of DAMSM phase and GAN training phase from here: https://drive.google.com/drive/folders/1nU369cbOBtmiduBeTd4N9nQP1mD4HDPv?usp=share_link","metadata":{}},{"cell_type":"markdown","source":"**RoBERTa +SN**","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/mansivv9/Text_to_image_synthesis_Major_Project.git","metadata":{"execution":{"iopub.status.busy":"2023-05-12T04:24:27.793395Z","iopub.execute_input":"2023-05-12T04:24:27.793743Z","iopub.status.idle":"2023-05-12T04:24:29.540222Z","shell.execute_reply.started":"2023-05-12T04:24:27.793711Z","shell.execute_reply":"2023-05-12T04:24:29.538778Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'Text_to_image_synthesis_Major_Project'...\nremote: Enumerating objects: 206, done.\u001b[K\nremote: Counting objects: 100% (120/120), done.\u001b[K (76/120)\u001b[K\nremote: Compressing objects: 100% (98/98), done.\u001b[K\nremote: Total 206 (delta 44), reused 69 (delta 22), pack-reused 86\u001b[K\nReceiving objects: 100% (206/206), 491.48 KiB | 8.94 MiB/s, done.\nResolving deltas: 100% (67/67), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.chdir('/kaggle/working/Text_to_image_synthesis_Major_Project/AttnGAN+CL+SN+RoBERTa(AttnGAN_V2)')","metadata":{"execution":{"iopub.status.busy":"2023-05-12T04:24:32.228822Z","iopub.execute_input":"2023-05-12T04:24:32.229259Z","iopub.status.idle":"2023-05-12T04:24:32.234785Z","shell.execute_reply.started":"2023-05-12T04:24:32.229224Z","shell.execute_reply":"2023-05-12T04:24:32.233783Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"%%writefile code/pretrain_DAMSM.py\nfrom __future__ import print_function\n\nfrom miscc.utils import mkdir_p\nfrom miscc.utils import build_super_images\nfrom miscc.losses import sent_loss, words_loss\nfrom miscc.config import cfg, cfg_from_file\n\nfrom model import RNN_ENCODER, CNN_ENCODER\n\nimport os\nimport sys\nimport time\nimport random\nimport pprint\nimport datetime\nimport dateutil.tz\nimport argparse\nimport numpy as np\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as transforms\n# from torch.nn.utils.rnn import pad_packed_sequence\n\nfrom transformers import RobertaModel\n\n\ndir_path = (os.path.abspath(os.path.join(os.path.realpath(__file__), './.')))\nsys.path.append(dir_path)\n\n\nUPDATE_INTERVAL = 50\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Train a DAMSM network')\n    parser.add_argument('--cfg', dest='cfg_file',\n                        help='optional config file',\n                        default='cfg/DAMSM/bird.yml', type=str)\n    parser.add_argument('--gpu', dest='gpu_id', type=int, default=0)\n    parser.add_argument('--data_dir', dest='data_dir', type=str, default='')\n    parser.add_argument('--manualSeed', type=int, help='manual seed')\n    args = parser.parse_args()\n    return args\n\n\ndef train( dataloader, cnn_model, nlp_model, batch_size,\n           labels, optimizer, epoch, ixtoword, image_dir ):\n    \n    if cfg.DATASET_NAME=='birds':\n        from datasets import TextDataset\n        from datasets import prepare_data\n    if cfg.DATASET_NAME=='flowers':\n        from datasets_flowers import TextDataset\n        from datasets_flowers import prepare_data\n        \n    cnn_model.train()\n    nlp_model.train()\n    s_total_loss0 = 0\n    s_total_loss1 = 0\n    w_total_loss0 = 0\n    w_total_loss1 = 0\n    count = (epoch + 1) * len(dataloader)\n    start_time = time.time()\n    for step, data in enumerate(dataloader, 0):\n        # print('step', step)\n        nlp_model.zero_grad()\n        cnn_model.zero_grad()\n\n        imgs, imgs_2, captions, cap_lens, class_ids, keys, captions_2, cap_lens_2, class_ids_2, \\\n        sort_ind, sort_ind_2 = prepare_data(data)\n\n\n        # words_features: batch_size x nef x 17 x 17\n        # sent_code: batch_size x nef\n        words_features, sent_code = cnn_model(imgs[-1])\n        # print( words_features.shape, sent_code.shape )\n        # --> batch_size x nef x 17*17\n        nef, att_sze = words_features.size(1), words_features.size(2)\n        # words_features = words_features.view(batch_size, nef, -1)\n\n        # Forward Prop:\n        # inputs:\n        #   captions: torch.LongTensor of ids of size batch x n_steps\n        # outputs:\n        #   words_emb: batch_size x nef x seq_len\n        #   sent_emb: batch_size x nef\n        \n        words_emb = nlp_model( captions )[0].transpose(1, 2).contiguous()\n        sent_emb = words_emb[ :, :, -1 ].contiguous()\n            # sent_emb = sent_emb.view(batch_size, -1)\n        # print( words_emb.shape, sent_emb.shape )\n\n        # Compute Loss:\n        # NOTE: the ideal loss for Transformer may be different than that for bi-directional LSTM\n        w_loss0, w_loss1, attn_maps = words_loss( words_features, words_emb, labels,\n                                                  cap_lens, class_ids, batch_size )\n        w_total_loss0 += w_loss0.data\n        w_total_loss1 += w_loss1.data\n        loss = w_loss0 + w_loss1\n\n        s_loss0, s_loss1 = \\\n            sent_loss( sent_code, sent_emb, labels, class_ids, batch_size )\n        loss += s_loss0 + s_loss1\n        s_total_loss0 += s_loss0.data\n        s_total_loss1 += s_loss1.data\n        #\n        # Backprop:\n        loss.backward()\n        #\n        # `clip_grad_norm` helps prevent\n        # the exploding gradient problem in RNNs / LSTMs.\n        optimizer.step()\n\n        if step % UPDATE_INTERVAL == 0:\n            count = epoch * len(dataloader) + step\n\n            # print(  s_total_loss0, s_total_loss1 )\n            s_cur_loss0 = s_total_loss0.item() / UPDATE_INTERVAL\n            s_cur_loss1 = s_total_loss1.item() / UPDATE_INTERVAL\n\n            # print(  w_total_loss0, w_total_loss1 )\n            w_cur_loss0 = w_total_loss0.item() / UPDATE_INTERVAL\n            w_cur_loss1 = w_total_loss1.item() / UPDATE_INTERVAL\n\n            elapsed = time.time() - start_time\n            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | '\n                  's_loss {:5.5f} {:5.5f} | '\n                  'w_loss {:5.5f} {:5.5f}'\n                  .format(epoch, step, len(dataloader),\n                          elapsed * 1000. / UPDATE_INTERVAL,\n                          s_cur_loss0, s_cur_loss1,\n                          w_cur_loss0, w_cur_loss1))\n            s_total_loss0 = 0\n            s_total_loss1 = 0\n            w_total_loss0 = 0\n            w_total_loss1 = 0\n            start_time = time.time()\n\n            # Attention Maps\n            img_set, _ = \\\n                build_super_images(imgs[-1].cpu(), captions,\n                                   ixtoword, attn_maps, att_sze)\n            if img_set is not None:\n                im = Image.fromarray(img_set)\n                fullpath = '%s/attention_maps%d.png' % (image_dir, step)\n                im.save(fullpath)\n    return count\n\n\ndef evaluate(dataloader, cnn_model, nlp_model, batch_size):\n    cnn_model.eval()\n    nlp_model.eval()    \n    s_total_loss = 0\n    w_total_loss = 0\n    if cfg.DATASET_NAME=='birds':\n        from datasets import TextDataset\n        from datasets import prepare_data\n    if cfg.DATASET_NAME=='flowers':\n        from datasets_flowers import TextDataset\n        from datasets_flowers import prepare_data\n    \n    for step, data in enumerate(dataloader, 0):\n        real_imgs, imgs_2, captions, cap_lens, class_ids, keys, captions_2, cap_lens_2, class_ids_2, \\\n        sort_ind, sort_ind_2 = prepare_data(data)\n\n        words_features, sent_code = cnn_model(real_imgs[-1])\n        # nef = words_features.size(1)\n        # words_features = words_features.view(batch_size, nef, -1)\n\n        words_emb = nlp_model( captions )[0].transpose(1, 2).contiguous()\n        sent_emb = words_emb[ :, :, -1 ].contiguous()\n            # sent_emb = sent_emb.view(batch_size, -1)\n\n        w_loss0, w_loss1, attn = words_loss( words_features, words_emb, labels,\n                                             cap_lens, class_ids, batch_size )\n        w_total_loss += ( w_loss0 + w_loss1 ).data\n\n        s_loss0, s_loss1 = \\\n            sent_loss( sent_code, sent_emb, labels, class_ids, batch_size )\n        s_total_loss += ( s_loss0 + s_loss1 ).data\n\n        if step == 50:\n            break\n\n    s_cur_loss = s_total_loss.item() / step\n    w_cur_loss = w_total_loss.item() / step\n\n    return s_cur_loss, w_cur_loss\n\n\ndef build_models():\n    # build model ############################################################\n    text_encoder = RobertaModel.from_pretrained('roberta-base')\n\n    image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)\n\n    labels = Variable(torch.LongTensor(range(batch_size)))\n    start_epoch = 0\n    if cfg.TRAIN.NET_E:\n        state_dict = torch.load(cfg.TRAIN.NET_E)\n        text_encoder.load_state_dict(state_dict)\n        print('Load ', cfg.TRAIN.NET_E)\n                                                      # output_hidden_states = True )\n          #\n        name = cfg.TRAIN.NET_E.replace( 'text_encoder', 'image_encoder' )\n        state_dict = torch.load(name)\n        image_encoder.load_state_dict(state_dict)\n        print('Load ', name)\n\n        istart = cfg.TRAIN.NET_E.rfind('_') + 8\n        iend = cfg.TRAIN.NET_E.rfind('.')\n        start_epoch = cfg.TRAIN.NET_E[istart:iend]\n        start_epoch = int(start_epoch) + 1\n\n    if cfg.CUDA:\n        text_encoder = text_encoder.cuda()\n        image_encoder = image_encoder.cuda()\n        labels = labels.cuda()\n\n    return text_encoder, image_encoder, labels, start_epoch\n\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    if args.cfg_file is not None:\n        cfg_from_file(args.cfg_file)\n\n    if args.gpu_id == -1:\n        cfg.CUDA = False\n    else:\n        cfg.GPU_ID = args.gpu_id\n\n    if args.data_dir != '':\n        cfg.DATA_DIR = args.data_dir\n    print('Using config:')\n    pprint.pprint(cfg)\n\n    if not cfg.TRAIN.FLAG:\n        args.manualSeed = 100\n    elif args.manualSeed is None:\n        args.manualSeed = random.randint(1, 10000)\n    random.seed(args.manualSeed)\n    np.random.seed(args.manualSeed)\n    torch.manual_seed(args.manualSeed)\n    if cfg.CUDA:\n        torch.cuda.manual_seed_all(args.manualSeed)\n    if cfg.DATASET_NAME=='birds':\n        from datasets import TextDataset\n        from datasets import prepare_data\n    if cfg.DATASET_NAME=='flowers':\n        from datasets_flowers import TextDataset\n        from datasets_flowers import prepare_data\n\n    ##########################################################################\n    now = datetime.datetime.now(dateutil.tz.tzlocal())\n    timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n    output_dir = 'damsm_output/%s_%s_%s' % \\\n        (cfg.DATASET_NAME, cfg.CONFIG_NAME, timestamp)\n\n    model_dir = os.path.join(output_dir, 'Model')\n    image_dir = os.path.join(output_dir, 'Image')\n    mkdir_p(model_dir)\n    mkdir_p(image_dir)\n\n    torch.cuda.set_device(cfg.GPU_ID)\n    cudnn.benchmark = True\n\n    # Get data loader ##################################################\n    imsize = cfg.TREE.BASE_SIZE * (2 ** (cfg.TREE.BRANCH_NUM-1))\n    batch_size = cfg.TRAIN.BATCH_SIZE\n    image_transform = transforms.Compose([\n        transforms.Resize(int(imsize * 76 / 64)),\n        transforms.RandomCrop(imsize),\n        transforms.RandomHorizontalFlip()])\n    dataset = TextDataset(cfg.DATA_DIR, 'train',\n                          base_size=cfg.TREE.BASE_SIZE,\n                          transform=image_transform)\n\n    print(dataset.n_words, dataset.embeddings_num)\n    assert dataset\n    dataloader = torch.utils.data.DataLoader(\n        dataset, batch_size=batch_size, drop_last=True,\n        shuffle=True, num_workers=int(cfg.WORKERS))\n\n    # # validation data #\n    dataset_val = TextDataset(cfg.DATA_DIR, 'test',\n                              base_size=cfg.TREE.BASE_SIZE,\n                              transform=image_transform)\n    dataloader_val = torch.utils.data.DataLoader(\n        dataset_val, batch_size=batch_size, drop_last=True,\n        shuffle=True, num_workers=int(cfg.WORKERS))\n\n    # Train ##############################################################\n    text_encoder, image_encoder, labels, start_epoch = build_models()\n    para = list(text_encoder.parameters())\n    for v in image_encoder.parameters():\n        if v.requires_grad:\n            para.append(v)\n    # optimizer = optim.Adam(para, lr=cfg.TRAIN.ENCODER_LR, betas=(0.5, 0.999))\n    # At any point you can hit Ctrl + C to break out of training early.\n    try:\n        lr = cfg.TRAIN.ENCODER_LR\n        for epoch in range(start_epoch, cfg.TRAIN.MAX_EPOCH):\n            optimizer = optim.Adam(para, lr=lr, betas=(0.5, 0.999))\n            epoch_start_time = time.time()\n            count = train(dataloader, image_encoder, text_encoder,\n                          batch_size, labels, optimizer, epoch,\n                          dataset.ixtoword, image_dir)\n            print('-' * 89)\n            if len(dataloader_val) > 0:\n                s_loss, w_loss = evaluate(dataloader_val, image_encoder,\n                                          text_encoder, batch_size)\n                print('| end epoch {:3d} | valid loss '\n                      '{:5.5f} {:5.5f} | lr {:.8f}|'\n                      .format(epoch, s_loss, w_loss, lr))\n            print('-' * 89)\n            if lr > cfg.TRAIN.ENCODER_LR/10.:\n                lr *= 0.98\n\n            if (epoch % cfg.TRAIN.SNAPSHOT_INTERVAL == 0 or\n                epoch == cfg.TRAIN.MAX_EPOCH):\n                torch.save(image_encoder.state_dict(),\n                           '%s/image_encoder%d.pth' % (model_dir, epoch))\n                torch.save(text_encoder.state_dict(),\n                           '%s/text_encoder%d.pth' % (model_dir, epoch))\n                print('Save G/Ds models.')\n    except KeyboardInterrupt:\n        print('-' * 89)\n        print('Exiting from training early')","metadata":{"execution":{"iopub.status.busy":"2023-05-12T04:40:32.700356Z","iopub.execute_input":"2023-05-12T04:40:32.700783Z","iopub.status.idle":"2023-05-12T04:40:32.718166Z","shell.execute_reply.started":"2023-05-12T04:40:32.700747Z","shell.execute_reply":"2023-05-12T04:40:32.716996Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Overwriting code/pretrain_DAMSM.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile code/trainer.py\nfrom __future__ import print_function\nfrom six.moves import range\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\n\nfrom PIL import Image\n\nfrom miscc.config import cfg\nfrom miscc.utils import mkdir_p\nfrom miscc.utils import build_super_images, build_super_images2\nfrom miscc.utils import weights_init, load_params, copy_G_params\nfrom model import G_DCGAN, G_NET\nfrom model import RNN_ENCODER, CNN_ENCODER\n\nfrom miscc.losses import words_loss\nfrom miscc.losses import discriminator_loss, generator_loss, KL_loss\nimport os\nimport time\nimport numpy as np\nimport sys\n\nfrom transformers import RobertaModel\n\nfrom torch.utils.tensorboard import SummaryWriter\n\ndef l2norm(X, dim, eps=1e-8):\n    \"\"\"L2-normalize columns of X\n    \"\"\"\n    norm = torch.pow(X, 2).sum(dim=dim, keepdim=True).sqrt() + eps\n    X = torch.div(X, norm)\n    return X\n\n\n# ################# Text to image task############################ #\nclass condGANTrainer(object):\n    def __init__(self, output_dir, data_loader, n_words, ixtoword, dataset):\n        if cfg.TRAIN.FLAG:\n            self.model_dir = os.path.join(output_dir, 'Model')\n            self.image_dir = os.path.join(output_dir, 'Image')\n            mkdir_p(self.model_dir)\n            mkdir_p(self.image_dir)\n\n        torch.cuda.set_device(cfg.GPU_ID)\n        cudnn.benchmark = True\n\n        self.batch_size = cfg.TRAIN.BATCH_SIZE\n        self.max_epoch = cfg.TRAIN.MAX_EPOCH\n        self.snapshot_interval = cfg.TRAIN.SNAPSHOT_INTERVAL\n\n        self.n_words = n_words\n        self.ixtoword = ixtoword\n        self.data_loader = data_loader\n        self.num_batches = len(self.data_loader)\n        self.dataset = dataset\n        self.writer = SummaryWriter('runs/visualize')\n\n\n    def build_models(self):\n        # ###################encoders######################################## #\n        if cfg.TRAIN.NET_E == '':\n            print('Error: no pretrained text-image encoders')\n            return\n\n        image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)\n        img_encoder_path = cfg.TRAIN.NET_E.replace('text_encoder', 'image_encoder')\n        state_dict = \\\n            torch.load(img_encoder_path, map_location=lambda storage, loc: storage)\n        image_encoder.load_state_dict(state_dict)\n        for p in image_encoder.parameters():\n            p.requires_grad = False\n        print('Load image encoder from:', img_encoder_path)\n        image_encoder.eval()\n\n        text_encoder = RobertaModel.from_pretrained(\"roberta-base\")\n        state_dict = \\\n            torch.load(cfg.TRAIN.NET_E,\n                       map_location=lambda storage, loc: storage)\n        text_encoder.load_state_dict(state_dict)\n        for p in text_encoder.parameters():\n            p.requires_grad = False\n        print('Load text encoder from:', cfg.TRAIN.NET_E)\n        text_encoder.eval()\n\n        # #######################generator and discriminators############## #\n        netsD = []\n        if cfg.GAN.B_DCGAN:\n            if cfg.TREE.BRANCH_NUM ==1:\n                from model import D_NET64 as D_NET\n            elif cfg.TREE.BRANCH_NUM == 2:\n                from model import D_NET128 as D_NET\n            else:  # cfg.TREE.BRANCH_NUM == 3:\n                from model import D_NET256 as D_NET\n            # TODO: elif cfg.TREE.BRANCH_NUM > 3:\n            netG = G_DCGAN()\n            netsD = [D_NET(b_jcu=False)]\n        else:\n            from model import D_NET64, D_NET128, D_NET256\n            netG = G_NET()\n            if cfg.TREE.BRANCH_NUM > 0:\n                netsD.append(D_NET64())\n            if cfg.TREE.BRANCH_NUM > 1:\n                netsD.append(D_NET128())\n            if cfg.TREE.BRANCH_NUM > 2:\n                netsD.append(D_NET256())\n            # TODO: if cfg.TREE.BRANCH_NUM > 3:\n        netG.apply(weights_init)\n        # print(netG)\n        for i in range(len(netsD)):\n            netsD[i].apply(weights_init)\n            # print(netsD[i])\n        print('# of netsD', len(netsD))\n        #\n        epoch = 0\n        if cfg.TRAIN.NET_G != '':\n            state_dict = \\\n                torch.load(cfg.TRAIN.NET_G, map_location=lambda storage, loc: storage)\n            netG.load_state_dict(state_dict)\n            print('Load G from: ', cfg.TRAIN.NET_G)\n            istart = cfg.TRAIN.NET_G.rfind('_') + 1\n            iend = cfg.TRAIN.NET_G.rfind('.')\n            epoch = cfg.TRAIN.NET_G[istart:iend]\n            epoch = int(epoch) + 1\n            if cfg.TRAIN.B_NET_D:\n                Gname = cfg.TRAIN.NET_G\n                for i in range(len(netsD)):\n                    s_tmp = Gname[:Gname.rfind('/')]\n                    Dname = '%s/netD%d.pth' % (s_tmp, i)\n                    print('Load D from: ', Dname)\n                    state_dict = \\\n                        torch.load(Dname, map_location=lambda storage, loc: storage)\n                    netsD[i].load_state_dict(state_dict)\n        # ########################################################### #\n        if cfg.CUDA:\n            text_encoder = text_encoder.cuda()\n            image_encoder = image_encoder.cuda()\n            netG.cuda()\n            for i in range(len(netsD)):\n                netsD[i].cuda()\n        return [text_encoder, image_encoder, netG, netsD, epoch]\n\n    def define_optimizers(self, netG, netsD):\n        optimizersD = []\n        num_Ds = len(netsD)\n        for i in range(num_Ds):\n            opt = optim.Adam(netsD[i].parameters(),\n                             lr=cfg.TRAIN.DISCRIMINATOR_LR,\n                             betas=(0.5, 0.999))\n            optimizersD.append(opt)\n\n        optimizerG = optim.Adam(netG.parameters(),\n                                lr=cfg.TRAIN.GENERATOR_LR,\n                                betas=(0.5, 0.999))\n\n        return optimizerG, optimizersD\n\n    def prepare_labels(self):\n        batch_size = self.batch_size\n        real_labels = Variable(torch.FloatTensor(batch_size).fill_(1))\n        fake_labels = Variable(torch.FloatTensor(batch_size).fill_(0))\n        match_labels = Variable(torch.LongTensor(range(batch_size)))\n        if cfg.CUDA:\n            real_labels = real_labels.cuda()\n            fake_labels = fake_labels.cuda()\n            match_labels = match_labels.cuda()\n\n        return real_labels, fake_labels, match_labels\n\n    def save_model(self, netG, avg_param_G, netsD, epoch):\n        backup_para = copy_G_params(netG)\n        load_params(netG, avg_param_G)\n        torch.save(netG.state_dict(),\n            '%s/netG_epoch_%d.pth' % (self.model_dir, epoch))\n        load_params(netG, backup_para)\n        #\n        for i in range(len(netsD)):\n            netD = netsD[i]\n            torch.save(netD.state_dict(),\n                '%s/netD%d.pth' % (self.model_dir, i))\n        print('Save G/Ds models.')\n\n    def set_requires_grad_value(self, models_list, brequires):\n        for i in range(len(models_list)):\n            for p in models_list[i].parameters():\n                p.requires_grad = brequires\n\n    def save_img_results(self, netG, noise, sent_emb, words_embs, mask,\n                         image_encoder, captions, cap_lens,\n                         gen_iterations, name='current'):\n        # Save images\n        fake_imgs, attention_maps, _, _ = netG(noise, sent_emb, words_embs, mask)\n        for i in range(len(attention_maps)):\n            if len(fake_imgs) > 1:\n                img = fake_imgs[i + 1].detach().cpu()\n                lr_img = fake_imgs[i].detach().cpu()\n            else:\n                img = fake_imgs[0].detach().cpu()\n                lr_img = None\n            attn_maps = attention_maps[i]\n            att_sze = attn_maps.size(2)\n            img_set, _ = \\\n                build_super_images(img, captions, self.ixtoword,\n                                   attn_maps, att_sze, lr_imgs=lr_img)\n            if img_set is not None:\n                im = Image.fromarray(img_set)\n                fullpath = '%s/G_%s_%d_%d.png'\\\n                    % (self.image_dir, name, gen_iterations, i)\n                im.save(fullpath)\n\n        # for i in range(len(netsD)):\n        i = -1\n        img = fake_imgs[i].detach()\n        region_features, _ = image_encoder(img)\n        att_sze = region_features.size(2)\n        _, _, att_maps = words_loss(region_features.detach(),\n                                    words_embs.detach(),\n                                    None, cap_lens,\n                                    None, self.batch_size)\n        img_set, _ = \\\n            build_super_images(fake_imgs[i].detach().cpu(),\n                               captions, self.ixtoword, att_maps, att_sze)\n        if img_set is not None:\n            im = Image.fromarray(img_set)\n            fullpath = '%s/D_%s_%d.png'\\\n                % (self.image_dir, name, gen_iterations)\n            im.save(fullpath)\n\n    def train(self):\n        text_encoder, image_encoder, netG, netsD, start_epoch = self.build_models()\n        avg_param_G = copy_G_params(netG)\n        optimizerG, optimizersD = self.define_optimizers(netG, netsD)\n        real_labels, fake_labels, match_labels = self.prepare_labels()\n\n        #real_labels_2, fake_labels_2, match_labels_2 = self.prepare_labels()\n\n        batch_size = self.batch_size\n        nz = cfg.GAN.Z_DIM\n        noise = Variable(torch.FloatTensor(batch_size, nz))\n        fixed_noise = Variable(torch.FloatTensor(batch_size, nz).normal_(0, 1))\n        if cfg.CUDA:\n            noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n\n        gen_iterations = 0\n\n        #mask = mask_correlated_samples(self)\n\n        #temperature = 0.5\n        #device = noise.get_device()\n        #criterion = NT_Xent(batch_size, temperature, mask, device)\n        if cfg.DATASET_NAME == 'birds':\n            from datasets import prepare_data\n        if cfg.DATASET_NAME == 'flowers':\n            from datasets_flowers import prepare_data\n\n        # gen_iterations = start_epoch * self.num_batches\n        for epoch in range(start_epoch, self.max_epoch):\n            start_t = time.time()\n\n            data_iter = iter(self.data_loader)\n            step = 0\n\n            D_total_loss = 0\n            G_total_loss = 0\n\n            while step < self.num_batches:\n                # reset requires_grad to be trainable for all Ds\n                # self.set_requires_grad_value(netsD, True)\n\n                ######################################################\n                # (1) Prepare training data and Compute text embeddings\n                ######################################################\n                data = next(data_iter)\n                imgs, imgs_2, captions, cap_lens, class_ids, keys, captions_2, cap_lens_2, class_ids_2, \\\n                sort_ind, sort_ind_2 = prepare_data(data)\n\n                words_embs = text_encoder( captions )[0].transpose(1, 2).contiguous()\n                    #words_embs = torch.Tensor(words_embs).cuda()\n                sent_emb = words_embs[ :, :, -1 ].contiguous()\n                words_embs, sent_emb = words_embs.detach(), sent_emb.detach()\n                mask = (captions == 0)\n                num_words = words_embs.size(2)\n                if mask.size(1) > num_words:\n                    mask = mask[:, :num_words]\n\n                #words_embs_2 = text_encoder( captions_2 )[0].transpose(1, 2).contiguous()\n                    #words_embs = torch.Tensor(words_embs).cuda()\n                #sent_emb_2 = words_embs_2[ :, :, -1 ].contiguous()\n                #words_embs_2, sent_emb_2 = words_embs_2.detach(), sent_emb_2.detach()\n                #mask_2 = (captions_2 == 0)\n                #num_words_2 = words_embs_2.size(2)\n                #if mask_2.size(1) > num_words_2:\n                 #   mask_2 = mask_2[:, :num_words_2]\n\n                #######################################################\n                # (2) Generate fake images\n                ######################################################\n                noise.data.normal_(0, 1)\n                fake_imgs, _, mu, logvar = netG(noise, sent_emb, words_embs, mask)\n                #fake_imgs_2, _, mu_2, logvar_2 = netG(noise, sent_emb_2, words_embs_2, mask_2)\n\n                #######################################################\n                # (3) Update D network\n                ######################################################\n                errD_total = 0\n                D_logs = ''\n                for i in range(len(netsD)):\n                    netsD[i].zero_grad()\n                    errD = discriminator_loss(netsD[i], imgs[i], fake_imgs[i],\n                                              sent_emb, real_labels, fake_labels)\n                    #errD_2 = discriminator_loss(netsD[i], imgs_2[i], fake_imgs_2[i],\n                    #                            sent_emb_2, real_labels_2, fake_labels_2)\n                    #errD += errD_2\n\n                    # backward and update parameters\n                    errD.backward()\n                    optimizersD[i].step()\n                    errD_total += errD\n                    D_logs += 'errD%d: %.2f ' % (i, errD.item())\n\n                #######################################################\n                # (4) Update G network: maximize log(D(G(z)))\n                ######################################################\n                # compute total loss for training G\n                step += 1\n                gen_iterations += 1\n\n                # do not need to compute gradient for Ds\n                # self.set_requires_grad_value(netsD, False)\n                netG.zero_grad()\n                errG_total, G_logs, cnn_code = \\\n                    generator_loss(netsD, image_encoder, fake_imgs, real_labels,\n                                   words_embs, sent_emb, match_labels, cap_lens, class_ids)\n                kl_loss = KL_loss(mu, logvar)\n                errG_total += kl_loss\n                G_logs += 'kl_loss: %.2f ' % kl_loss.item()\n\n                \"\"\"errG_total_2, G_logs_2, cnn_code_2 = \\\n                    generator_loss(netsD, image_encoder, fake_imgs_2, real_labels_2,\n                                   words_embs_2, sent_emb_2, match_labels_2, cap_lens_2, class_ids_2)\n                kl_loss_2 = KL_loss(mu_2, logvar_2)\n                errG_total_2 += kl_loss_2\n                G_logs_2 += 'kl_loss: %.2f ' % kl_loss_2.item()\n\n                errG_total += errG_total_2\n\n                _, ori_indices = torch.sort(sort_ind, 0)\n                _, ori_indices_2 = torch.sort(sort_ind_2, 0)\n\n                total_contra_loss = 0\"\"\"\n                i = -1\n                \n                \"\"\"cnn_code = cnn_code[ori_indices]\n                cnn_code_2 = cnn_code_2[ori_indices_2]\n\n                cnn_code = l2norm(cnn_code, dim=1)\n                cnn_code_2 = l2norm(cnn_code_2, dim=1)\n\n                contrative_loss = criterion(cnn_code, cnn_code_2)\n                total_contra_loss += contrative_loss *  0.2\n                G_logs += 'contrative_loss: %.2f ' % total_contra_loss.item()\n                errG_total += total_contra_loss\"\"\"\n                # backward and update parameters\n                errG_total.backward()\n                optimizerG.step()\n                for p, avg_p in zip(netG.parameters(), avg_param_G):\n                    avg_p.mul_(0.999).add_(0.001, p.data)\n\n                if gen_iterations % 100 == 0:\n                    print(D_logs + '\\n' + G_logs +'\\n')\n                # save images\n                if gen_iterations % 1000 == 0:\n                    backup_para = copy_G_params(netG)\n                    load_params(netG, avg_param_G)\n                    self.save_img_results(netG, fixed_noise, sent_emb,\n                                           words_embs, mask, image_encoder,\n                                           captions, cap_lens, epoch, name='average')\n                    load_params(netG, backup_para)\n                    #\n                    \"\"\"\n                    self.save_img_results(netG, fixed_noise, sent_emb,\n                                           words_embs, mask, image_encoder,\n                                           captions, cap_lens,\n                                           epoch, name='current')\n                    \"\"\"\n                #D_total_loss += errD_total.item()\n                #G_total_loss += errG_total.item()\n\n            end_t = time.time()\n\n            print('''[%d/%d][%d]\n                  Loss_D: %.2f Loss_G: %.2f Time: %.2fs'''\n                  % (epoch, self.max_epoch, self.num_batches,\n                     errD_total.item(), errG_total.item(),\n                     end_t - start_t))\n\n            if epoch % cfg.TRAIN.SNAPSHOT_INTERVAL == 0:  # and epoch != 0:\n                self.save_model(netG, avg_param_G, netsD, epoch)\n\n            D_total_loss = D_total_loss / step\n            G_total_loss = G_total_loss / step\n            # self.writer.add_scalar('Loss_D', D_total_loss , epoch  + 1)\n            # self.writer.add_scalar('Loss_G', G_total_loss , epoch  + 1)\n            self.writer.add_scalars('Loss_D and Loss_G', {'Loss_D': D_total_loss, 'Loss_G': G_total_loss}, epoch  + 1)\n\n        self.writer.close()\n\n        self.save_model(netG, avg_param_G, netsD, self.max_epoch)\n\n    def save_singleimages(self, images, filenames, save_dir,\n                          split_dir, sentenceID=0):\n        for i in range(images.size(0)):\n            s_tmp = '%s/single_samples/%s/%s' %\\\n                (save_dir, split_dir, filenames[i])\n            folder = s_tmp[:s_tmp.rfind('/')]\n            if not os.path.isdir(folder):\n                print('Make a new folder: ', folder)\n                mkdir_p(folder)\n\n            fullpath = '%s_%d.jpg' % (s_tmp, sentenceID)\n            # range from [-1, 1] to [0, 1]\n            # img = (images[i] + 1.0) / 2\n            img = images[i].add(1).div(2).mul(255).clamp(0, 255).byte()\n            # range from [0, 1] to [0, 255]\n            ndarr = img.permute(1, 2, 0).data.cpu().numpy()\n            im = Image.fromarray(ndarr)\n            im.save(fullpath)\n\n    def sampling(self, split_dir):\n        if cfg.DATASET_NAME=='birds':\n            from datasets import prepare_data\n        if cfg.DATASET_NAME=='flowers':\n            from datasets_flowers import prepare_data\n        if cfg.TRAIN.NET_G == '':\n            print('Error: the path for morels is not found!')\n        else:\n            if split_dir == 'test':\n                split_dir = 'valid'\n            # Build and load the generator\n            if cfg.GAN.B_DCGAN:\n                netG = G_DCGAN()\n            else:\n                netG = G_NET()\n            netG.apply(weights_init)\n            netG.cuda()\n            netG.eval()\n\n            # load text encoder\n            text_encoder = RobertaModel.from_pretrained(\"roberta-base\")\n            state_dict = torch.load(cfg.TRAIN.NET_E, map_location=lambda storage, loc: storage)\n            text_encoder.load_state_dict(state_dict)\n            print('Load text encoder from:', cfg.TRAIN.NET_E)\n            text_encoder = text_encoder.cuda()\n            text_encoder.eval()\n\n            #load image encoder\n            image_encoder = CNN_ENCODER(cfg.TEXT.EMBEDDING_DIM)\n            img_encoder_path = cfg.TRAIN.NET_E.replace('text_encoder', 'image_encoder')\n            state_dict = torch.load(img_encoder_path, map_location=lambda storage, loc: storage)\n            image_encoder.load_state_dict(state_dict)\n            print('Load image encoder from:', img_encoder_path)\n            image_encoder = image_encoder.cuda()\n            image_encoder.eval()\n\n            batch_size = self.batch_size\n            nz = cfg.GAN.Z_DIM\n            noise = Variable(torch.FloatTensor(batch_size, nz), volatile=True)\n            noise = noise.cuda()\n\n            model_dir = cfg.TRAIN.NET_G\n            state_dict = torch.load(model_dir, map_location=lambda storage, loc: storage)\n            # state_dict = torch.load(cfg.TRAIN.NET_G)\n            netG.load_state_dict(state_dict)\n            print('Load G from: ', model_dir)\n\n            # the path to save generated images\n            s_tmp = 'val_gen_images'\n            save_dir = '%s/%s' % (s_tmp, split_dir)\n            mkdir_p(save_dir)\n\n            cnt = 0\n            R_count = 0\n            R = np.zeros(30000)\n            cont = True\n            for ii in range(11):  # (cfg.TEXT.CAPTIONS_PER_IMAGE):\n                if (cont == False):\n                    break\n                for step, data in enumerate(self.data_loader, 0):\n                    cnt += batch_size\n                    if (cont == False):\n                        break\n                    if step % 100 == 0:\n                       print('cnt: ', cnt)\n                    # if step > 50:\n                    #     break\n\n                    # imgs, captions, cap_lens, class_ids, keys = prepare_data(data)\n\n                    imgs, imgs_2, captions, cap_lens, class_ids, keys, captions_2, cap_lens_2, class_ids_2, \\\n                    sort_ind, sort_ind_2 = prepare_data(data)\n\n                    words_embs = text_encoder( captions )[0].transpose(1, 2).contiguous()\n                    sent_emb = words_embs[ :, :, -1 ].contiguous()\n                    words_embs, sent_emb = words_embs.detach(), sent_emb.detach()\n                    mask = (captions == 0)\n                    num_words = words_embs.size(2)\n                    if mask.size(1) > num_words:\n                        mask = mask[:, :num_words]\n\n                    #######################################################\n                    # (2) Generate fake images\n                    ######################################################\n                    noise.data.normal_(0, 1)\n                    fake_imgs, _, _, _ = netG(noise, sent_emb, words_embs, mask)\n                    for j in range(batch_size):\n                        s_tmp = '%s/single/%s' % (save_dir, keys[j])\n                        folder = s_tmp[:s_tmp.rfind('/')]\n                        if not os.path.isdir(folder):\n                            #print('Make a new folder: ', folder)\n                            mkdir_p(folder)\n                        k = -1\n                        # for k in range(len(fake_imgs)):\n                        im = fake_imgs[k][j].data.cpu().numpy()\n                        # [-1, 1] --> [0, 255]\n                        im = (im + 1.0) * 127.5\n                        im = im.astype(np.uint8)\n                        im = np.transpose(im, (1, 2, 0))\n                        im = Image.fromarray(im)\n                        fullpath = '%s_s%d_%d.png' % (s_tmp, k, ii)\n                        im.save(fullpath)\n\n                    _, cnn_code = image_encoder(fake_imgs[-1])\n\n                    for i in range(batch_size):\n                        mis_captions, mis_captions_len = self.dataset.get_mis_caption(class_ids[i])\n                        words_embs_t = text_encoder(mis_captions )[0].transpose(1, 2).contiguous()\n                        sent_emb_t = words_embs[ :, :, -1 ].contiguous()\n                        rnn_code = torch.cat((sent_emb[i, :].unsqueeze(0), sent_emb_t), 0)\n                        ### cnn_code = 1 * nef\n                        ### rnn_code = 100 * nef\n                        scores = torch.mm(cnn_code[i].unsqueeze(0), rnn_code.transpose(0, 1))  # 1* 100\n                        cnn_code_norm = torch.norm(cnn_code[i].unsqueeze(0), 2, dim=1, keepdim=True)\n                        rnn_code_norm = torch.norm(rnn_code, 2, dim=1, keepdim=True)\n                        norm = torch.mm(cnn_code_norm, rnn_code_norm.transpose(0, 1))\n                        scores0 = scores / norm.clamp(min=1e-8)\n                        if torch.argmax(scores0) == 0:\n                            R[R_count] = 1\n                        R_count += 1\n\n                    if R_count >= 30000:\n                        sum = np.zeros(10)\n                        np.random.shuffle(R)\n                        for i in range(10):\n                            sum[i] = np.average(R[i * 3000:(i + 1) * 3000 - 1])\n                        R_mean = np.average(sum)\n                        R_std = np.std(sum)\n                        print(\"R mean:{:.4f} std:{:.4f}\".format(R_mean, R_std))\n                        cont = False\n\n    def gen_example(self, data_dic):\n        if cfg.TRAIN.NET_G == '':\n            print('Error: the path for morels is not found!')\n        else:\n            # Build and load the generator\n            text_encoder = RobertaModel.from_pretrained(\"roberta-base\")\n            state_dict = \\\n                torch.load(cfg.TRAIN.NET_E, map_location=lambda storage, loc: storage)\n            text_encoder.load_state_dict(state_dict)\n            print('Load text encoder from:', cfg.TRAIN.NET_E)\n            text_encoder = text_encoder.cuda()\n            text_encoder.eval()\n\n            # the path to save generated images\n            if cfg.GAN.B_DCGAN:\n                netG = G_DCGAN()\n            else:\n                netG = G_NET()\n            s_tmp = 'example_images'\n            model_dir = cfg.TRAIN.NET_G\n            state_dict = \\\n                torch.load(model_dir, map_location=lambda storage, loc: storage)\n            netG.load_state_dict(state_dict)\n            print('Load G from: ', model_dir)\n            netG.cuda()\n            netG.eval()\n            for key in data_dic:\n                save_dir = '%s/%s' % (s_tmp, key)\n                mkdir_p(save_dir)\n                captions, cap_lens, sorted_indices = data_dic[key]\n\n                batch_size = captions.shape[0]\n                nz = cfg.GAN.Z_DIM\n                captions = Variable(torch.from_numpy(captions), volatile=True)\n                cap_lens = Variable(torch.from_numpy(cap_lens), volatile=True)\n\n                captions = captions.cuda()\n                cap_lens = cap_lens.cuda()\n                for i in range(1):  # 16\n                    noise = Variable(torch.FloatTensor(batch_size, nz), volatile=True)\n                    noise = noise.cuda()\n                    #######################################################\n                    # (1) Extract text embeddings\n                    ######################################################\n                    words_embs = text_encoder( captions )[0].transpose(1, 2).contiguous()\n                    sent_emb = words_embs[ :, :, -1 ].contiguous()\n                    mask = (captions == 0)\n                    #######################################################\n                    # (2) Generate fake images\n                    ######################################################\n                    noise.data.normal_(0, 1)\n                    fake_imgs, attention_maps, _, _ = netG(noise, sent_emb, words_embs, mask)\n                    # G attention\n                    cap_lens_np = cap_lens.cpu().data.numpy()\n                    for j in range(batch_size):\n                        save_name = '%s/%d_s_%d' % (save_dir, i, sorted_indices[j])\n                        for k in range(len(fake_imgs)):\n                            im = fake_imgs[k][j].data.cpu().numpy()\n                            im = (im + 1.0) * 127.5\n                            im = im.astype(np.uint8)\n                            # print('im', im.shape)\n                            im = np.transpose(im, (1, 2, 0))\n                            # print('im', im.shape)\n                            im = Image.fromarray(im)\n                            fullpath = '%s_g%d.png' % (save_name, k)\n                            im.save(fullpath)\n\n                        for k in range(len(attention_maps)):\n                            if len(fake_imgs) > 1:\n                                im = fake_imgs[k + 1].detach().cpu()\n                            else:\n                                im = fake_imgs[0].detach().cpu()\n                            attn_maps = attention_maps[k]\n                            att_sze = attn_maps.size(2)\n                            img_set, sentences = \\\n                                build_super_images2(im[j].unsqueeze(0),\n                                                    captions[j].unsqueeze(0),\n                                                    [cap_lens_np[j]], self.ixtoword,\n                                                    [attn_maps[j]], att_sze)\n                            if img_set is not None:\n                                im = Image.fromarray(img_set)\n                                fullpath = '%s_a%d.png' % (save_name, k)\n                                im.save(fullpath)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T05:48:18.082256Z","iopub.execute_input":"2023-05-12T05:48:18.082676Z","iopub.status.idle":"2023-05-12T05:48:18.114679Z","shell.execute_reply.started":"2023-05-12T05:48:18.082638Z","shell.execute_reply":"2023-05-12T05:48:18.113750Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Overwriting code/trainer.py\n","output_type":"stream"}]}]}